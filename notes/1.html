<!doctype html>
<html>
<head>
<meta charset='UTF-8'><meta name='viewport' content='width=device-width initial-scale=1'>
<title>L1web</title><style type='text/css'>html {overflow-x: initial !important;}:root { --bg-color:#ffffff; --text-color:#333333; --select-text-bg-color:#B5D6FC; --select-text-font-color:auto; --monospace:"Lucida Console",Consolas,"Courier",monospace; }
html { font-size: 14px; background-color: var(--bg-color); color: var(--text-color); font-family: "Helvetica Neue", Helvetica, Arial, sans-serif; -webkit-font-smoothing: antialiased; }
body { margin: 0px; padding: 0px; height: auto; bottom: 0px; top: 0px; left: 0px; right: 0px; font-size: 1rem; line-height: 1.42857; overflow-x: hidden; background: inherit; tab-size: 4; }
iframe { margin: auto; }
a.url { word-break: break-all; }
a:active, a:hover { outline: 0px; }
.in-text-selection, ::selection { text-shadow: none; background: var(--select-text-bg-color); color: var(--select-text-font-color); }
#write { margin: 0px auto; height: auto; width: inherit; word-break: normal; overflow-wrap: break-word; position: relative; white-space: normal; overflow-x: visible; padding-top: 40px; }
#write.first-line-indent p { text-indent: 2em; }
#write.first-line-indent li p, #write.first-line-indent p * { text-indent: 0px; }
#write.first-line-indent li { margin-left: 2em; }
.for-image #write { padding-left: 8px; padding-right: 8px; }
body.typora-export { padding-left: 30px; padding-right: 30px; }
.typora-export .footnote-line, .typora-export li, .typora-export p { white-space: pre-wrap; }
@media screen and (max-width: 500px) {
  body.typora-export { padding-left: 0px; padding-right: 0px; }
  #write { padding-left: 20px; padding-right: 20px; }
  .CodeMirror-sizer { margin-left: 0px !important; }
  .CodeMirror-gutters { display: none !important; }
}
#write li > figure:last-child { margin-bottom: 0.5rem; }
#write ol, #write ul { position: relative; }
img { max-width: 100%; vertical-align: middle; }
button, input, select, textarea { color: inherit; font: inherit; }
input[type="checkbox"], input[type="radio"] { line-height: normal; padding: 0px; }
*, ::after, ::before { box-sizing: border-box; }
#write h1, #write h2, #write h3, #write h4, #write h5, #write h6, #write p, #write pre { width: inherit; }
#write h1, #write h2, #write h3, #write h4, #write h5, #write h6, #write p { position: relative; }
p { line-height: inherit; }
h1, h2, h3, h4, h5, h6 { break-after: avoid-page; break-inside: avoid; orphans: 2; }
p { orphans: 4; }
h1 { font-size: 2rem; }
h2 { font-size: 1.8rem; }
h3 { font-size: 1.6rem; }
h4 { font-size: 1.4rem; }
h5 { font-size: 1.2rem; }
h6 { font-size: 1rem; }
.md-math-block, .md-rawblock, h1, h2, h3, h4, h5, h6, p { margin-top: 1rem; margin-bottom: 1rem; }
.hidden { display: none; }
.md-blockmeta { color: rgb(204, 204, 204); font-weight: 700; font-style: italic; }
a { cursor: pointer; }
sup.md-footnote { padding: 2px 4px; background-color: rgba(238, 238, 238, 0.7); color: rgb(85, 85, 85); border-radius: 4px; cursor: pointer; }
sup.md-footnote a, sup.md-footnote a:hover { color: inherit; text-transform: inherit; text-decoration: inherit; }
#write input[type="checkbox"] { cursor: pointer; width: inherit; height: inherit; }
figure { overflow-x: auto; margin: 1.2em 0px; max-width: calc(100% + 16px); padding: 0px; }
figure > table { margin: 0px !important; }
tr { break-inside: avoid; break-after: auto; }
thead { display: table-header-group; }
table { border-collapse: collapse; border-spacing: 0px; width: 100%; overflow: auto; break-inside: auto; text-align: left; }
table.md-table td { min-width: 32px; }
.CodeMirror-gutters { border-right: 0px; background-color: inherit; }
.CodeMirror-linenumber { user-select: none; }
.CodeMirror { text-align: left; }
.CodeMirror-placeholder { opacity: 0.3; }
.CodeMirror pre { padding: 0px 4px; }
.CodeMirror-lines { padding: 0px; }
div.hr:focus { cursor: none; }
#write pre { white-space: pre-wrap; }
#write.fences-no-line-wrapping pre { white-space: pre; }
#write pre.ty-contain-cm { white-space: normal; }
.CodeMirror-gutters { margin-right: 4px; }
.md-fences { font-size: 0.9rem; display: block; break-inside: avoid; text-align: left; overflow: visible; white-space: pre; background: inherit; position: relative !important; }
.md-diagram-panel { width: 100%; margin-top: 10px; text-align: center; padding-top: 0px; padding-bottom: 8px; overflow-x: auto; }
#write .md-fences.mock-cm { white-space: pre-wrap; }
.md-fences.md-fences-with-lineno { padding-left: 0px; }
#write.fences-no-line-wrapping .md-fences.mock-cm { white-space: pre; overflow-x: auto; }
.md-fences.mock-cm.md-fences-with-lineno { padding-left: 8px; }
.CodeMirror-line, twitterwidget { break-inside: avoid; }
.footnotes { opacity: 0.8; font-size: 0.9rem; margin-top: 1em; margin-bottom: 1em; }
.footnotes + .footnotes { margin-top: 0px; }
.md-reset { margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: top; background: 0px 0px; text-decoration: none; text-shadow: none; float: none; position: static; width: auto; height: auto; white-space: nowrap; cursor: inherit; -webkit-tap-highlight-color: transparent; line-height: normal; font-weight: 400; text-align: left; box-sizing: content-box; direction: ltr; }
li div { padding-top: 0px; }
blockquote { margin: 1rem 0px; }
li .mathjax-block, li p { margin: 0.5rem 0px; }
li { margin: 0px; position: relative; }
blockquote > :last-child { margin-bottom: 0px; }
blockquote > :first-child, li > :first-child { margin-top: 0px; }
.footnotes-area { color: rgb(136, 136, 136); margin-top: 0.714rem; padding-bottom: 0.143rem; white-space: normal; }
#write .footnote-line { white-space: pre-wrap; }
@media print {
  body, html { border: 1px solid transparent; height: 99%; break-after: avoid; break-before: avoid; }
  #write { margin-top: 0px; padding-top: 0px; border-color: transparent !important; }
  .typora-export * { -webkit-print-color-adjust: exact; }
  html.blink-to-pdf { font-size: 13px; }
  .typora-export #write { padding-left: 32px; padding-right: 32px; padding-bottom: 0px; break-after: avoid; }
  .typora-export #write::after { height: 0px; }
}
.footnote-line { margin-top: 0.714em; font-size: 0.7em; }
a img, img a { cursor: pointer; }
pre.md-meta-block { font-size: 0.8rem; min-height: 0.8rem; white-space: pre-wrap; background: rgb(204, 204, 204); display: block; overflow-x: hidden; }
p > .md-image:only-child:not(.md-img-error) img, p > img:only-child { display: block; margin: auto; }
p > .md-image:only-child { display: inline-block; width: 100%; }
#write .MathJax_Display { margin: 0.8em 0px 0px; }
.md-math-block { width: 100%; }
.md-math-block:not(:empty)::after { display: none; }
[contenteditable="true"]:active, [contenteditable="true"]:focus { outline: 0px; box-shadow: none; }
.md-task-list-item { position: relative; list-style-type: none; }
.task-list-item.md-task-list-item { padding-left: 0px; }
.md-task-list-item > input { position: absolute; top: 0px; left: 0px; margin-left: -1.2em; margin-top: calc(1em - 10px); border: none; }
.math { font-size: 1rem; }
.md-toc { min-height: 3.58rem; position: relative; font-size: 0.9rem; border-radius: 10px; }
.md-toc-content { position: relative; margin-left: 0px; }
.md-toc-content::after, .md-toc::after { display: none; }
.md-toc-item { display: block; color: rgb(65, 131, 196); }
.md-toc-item a { text-decoration: none; }
.md-toc-inner:hover { text-decoration: underline; }
.md-toc-inner { display: inline-block; cursor: pointer; }
.md-toc-h1 .md-toc-inner { margin-left: 0px; font-weight: 700; }
.md-toc-h2 .md-toc-inner { margin-left: 2em; }
.md-toc-h3 .md-toc-inner { margin-left: 4em; }
.md-toc-h4 .md-toc-inner { margin-left: 6em; }
.md-toc-h5 .md-toc-inner { margin-left: 8em; }
.md-toc-h6 .md-toc-inner { margin-left: 10em; }
@media screen and (max-width: 48em) {
  .md-toc-h3 .md-toc-inner { margin-left: 3.5em; }
  .md-toc-h4 .md-toc-inner { margin-left: 5em; }
  .md-toc-h5 .md-toc-inner { margin-left: 6.5em; }
  .md-toc-h6 .md-toc-inner { margin-left: 8em; }
}
a.md-toc-inner { font-size: inherit; font-style: inherit; font-weight: inherit; line-height: inherit; }
.footnote-line a:not(.reversefootnote) { color: inherit; }
.md-attr { display: none; }
.md-fn-count::after { content: "."; }
code, pre, samp, tt { font-family: var(--monospace); }
kbd { margin: 0px 0.1em; padding: 0.1em 0.6em; font-size: 0.8em; color: rgb(36, 39, 41); background: rgb(255, 255, 255); border: 1px solid rgb(173, 179, 185); border-radius: 3px; box-shadow: rgba(12, 13, 14, 0.2) 0px 1px 0px, rgb(255, 255, 255) 0px 0px 0px 2px inset; white-space: nowrap; vertical-align: middle; }
.md-comment { color: rgb(162, 127, 3); opacity: 0.8; font-family: var(--monospace); }
code { text-align: left; vertical-align: initial; }
a.md-print-anchor { white-space: pre !important; border-width: initial !important; border-style: none !important; border-color: initial !important; display: inline-block !important; position: absolute !important; width: 1px !important; right: 0px !important; outline: 0px !important; background: 0px 0px !important; text-decoration: initial !important; text-shadow: initial !important; }
.md-inline-math .MathJax_SVG .noError { display: none !important; }
.html-for-mac .inline-math-svg .MathJax_SVG { vertical-align: 0.2px; }
.md-math-block .MathJax_SVG_Display { text-align: center; margin: 0px; position: relative; text-indent: 0px; max-width: none; max-height: none; min-height: 0px; min-width: 100%; width: auto; overflow-y: hidden; display: block !important; }
.MathJax_SVG_Display, .md-inline-math .MathJax_SVG_Display { width: auto; margin: inherit; display: inline-block !important; }
.MathJax_SVG .MJX-monospace { font-family: var(--monospace); }
.MathJax_SVG .MJX-sans-serif { font-family: sans-serif; }
.MathJax_SVG { display: inline; font-style: normal; font-weight: 400; line-height: normal; zoom: 90%; text-indent: 0px; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; overflow-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; padding: 0px; margin: 0px; }
.MathJax_SVG * { transition: none 0s ease 0s; }
.MathJax_SVG_Display svg { vertical-align: middle !important; margin-bottom: 0px !important; margin-top: 0px !important; }
.os-windows.monocolor-emoji .md-emoji { font-family: "Segoe UI Symbol", sans-serif; }
.md-diagram-panel > svg { max-width: 100%; }
[lang="mermaid"] svg, [lang="flow"] svg { max-width: 100%; height: auto; }
[lang="mermaid"] .node text { font-size: 1rem; }
table tr th { border-bottom: 0px; }
video { max-width: 100%; display: block; margin: 0px auto; }
iframe { max-width: 100%; width: 100%; border: none; }
.highlight td, .highlight tr { border: 0px; }


.CodeMirror { height: auto; }
.CodeMirror.cm-s-inner { background: inherit; }
.CodeMirror-scroll { overflow: auto hidden; z-index: 3; }
.CodeMirror-gutter-filler, .CodeMirror-scrollbar-filler { background-color: rgb(255, 255, 255); }
.CodeMirror-gutters { border-right: 1px solid rgb(221, 221, 221); background: inherit; white-space: nowrap; }
.CodeMirror-linenumber { padding: 0px 3px 0px 5px; text-align: right; color: rgb(153, 153, 153); }
.cm-s-inner .cm-keyword { color: rgb(119, 0, 136); }
.cm-s-inner .cm-atom, .cm-s-inner.cm-atom { color: rgb(34, 17, 153); }
.cm-s-inner .cm-number { color: rgb(17, 102, 68); }
.cm-s-inner .cm-def { color: rgb(0, 0, 255); }
.cm-s-inner .cm-variable { color: rgb(0, 0, 0); }
.cm-s-inner .cm-variable-2 { color: rgb(0, 85, 170); }
.cm-s-inner .cm-variable-3 { color: rgb(0, 136, 85); }
.cm-s-inner .cm-string { color: rgb(170, 17, 17); }
.cm-s-inner .cm-property { color: rgb(0, 0, 0); }
.cm-s-inner .cm-operator { color: rgb(152, 26, 26); }
.cm-s-inner .cm-comment, .cm-s-inner.cm-comment { color: rgb(170, 85, 0); }
.cm-s-inner .cm-string-2 { color: rgb(255, 85, 0); }
.cm-s-inner .cm-meta { color: rgb(85, 85, 85); }
.cm-s-inner .cm-qualifier { color: rgb(85, 85, 85); }
.cm-s-inner .cm-builtin { color: rgb(51, 0, 170); }
.cm-s-inner .cm-bracket { color: rgb(153, 153, 119); }
.cm-s-inner .cm-tag { color: rgb(17, 119, 0); }
.cm-s-inner .cm-attribute { color: rgb(0, 0, 204); }
.cm-s-inner .cm-header, .cm-s-inner.cm-header { color: rgb(0, 0, 255); }
.cm-s-inner .cm-quote, .cm-s-inner.cm-quote { color: rgb(0, 153, 0); }
.cm-s-inner .cm-hr, .cm-s-inner.cm-hr { color: rgb(153, 153, 153); }
.cm-s-inner .cm-link, .cm-s-inner.cm-link { color: rgb(0, 0, 204); }
.cm-negative { color: rgb(221, 68, 68); }
.cm-positive { color: rgb(34, 153, 34); }
.cm-header, .cm-strong { font-weight: 700; }
.cm-del { text-decoration: line-through; }
.cm-em { font-style: italic; }
.cm-link { text-decoration: underline; }
.cm-error { color: red; }
.cm-invalidchar { color: red; }
.cm-constant { color: rgb(38, 139, 210); }
.cm-defined { color: rgb(181, 137, 0); }
div.CodeMirror span.CodeMirror-matchingbracket { color: rgb(0, 255, 0); }
div.CodeMirror span.CodeMirror-nonmatchingbracket { color: rgb(255, 34, 34); }
.cm-s-inner .CodeMirror-activeline-background { background: inherit; }
.CodeMirror { position: relative; overflow: hidden; }
.CodeMirror-scroll { height: 100%; outline: 0px; position: relative; box-sizing: content-box; background: inherit; }
.CodeMirror-sizer { position: relative; }
.CodeMirror-gutter-filler, .CodeMirror-hscrollbar, .CodeMirror-scrollbar-filler, .CodeMirror-vscrollbar { position: absolute; z-index: 6; display: none; }
.CodeMirror-vscrollbar { right: 0px; top: 0px; overflow: hidden; }
.CodeMirror-hscrollbar { bottom: 0px; left: 0px; overflow: hidden; }
.CodeMirror-scrollbar-filler { right: 0px; bottom: 0px; }
.CodeMirror-gutter-filler { left: 0px; bottom: 0px; }
.CodeMirror-gutters { position: absolute; left: 0px; top: 0px; padding-bottom: 30px; z-index: 3; }
.CodeMirror-gutter { white-space: normal; height: 100%; box-sizing: content-box; padding-bottom: 30px; margin-bottom: -32px; display: inline-block; }
.CodeMirror-gutter-wrapper { position: absolute; z-index: 4; background: 0px 0px !important; border: none !important; }
.CodeMirror-gutter-background { position: absolute; top: 0px; bottom: 0px; z-index: 4; }
.CodeMirror-gutter-elt { position: absolute; cursor: default; z-index: 4; }
.CodeMirror-lines { cursor: text; }
.CodeMirror pre { border-radius: 0px; border-width: 0px; background: 0px 0px; font-family: inherit; font-size: inherit; margin: 0px; white-space: pre; overflow-wrap: normal; color: inherit; z-index: 2; position: relative; overflow: visible; }
.CodeMirror-wrap pre { overflow-wrap: break-word; white-space: pre-wrap; word-break: normal; }
.CodeMirror-code pre { border-right: 30px solid transparent; width: fit-content; }
.CodeMirror-wrap .CodeMirror-code pre { border-right: none; width: auto; }
.CodeMirror-linebackground { position: absolute; left: 0px; right: 0px; top: 0px; bottom: 0px; z-index: 0; }
.CodeMirror-linewidget { position: relative; z-index: 2; overflow: auto; }
.CodeMirror-wrap .CodeMirror-scroll { overflow-x: hidden; }
.CodeMirror-measure { position: absolute; width: 100%; height: 0px; overflow: hidden; visibility: hidden; }
.CodeMirror-measure pre { position: static; }
.CodeMirror div.CodeMirror-cursor { position: absolute; visibility: hidden; border-right: none; width: 0px; }
.CodeMirror div.CodeMirror-cursor { visibility: hidden; }
.CodeMirror-focused div.CodeMirror-cursor { visibility: inherit; }
.cm-searching { background: rgba(255, 255, 0, 0.4); }
@media print {
  .CodeMirror div.CodeMirror-cursor { visibility: hidden; }
}


:root { --side-bar-bg-color: #fafafa; --control-text-color: #777; }
html { font-size: 16px; }
body { font-family: "Open Sans", "Clear Sans", "Helvetica Neue", Helvetica, Arial, sans-serif; color: rgb(51, 51, 51); line-height: 1.6; }
#write { max-width: 860px; margin: 0px auto; padding: 30px 30px 100px; }
#write > ul:first-child, #write > ol:first-child { margin-top: 30px; }
a { color: rgb(65, 131, 196); }
h1, h2, h3, h4, h5, h6 { position: relative; margin-top: 1rem; margin-bottom: 1rem; font-weight: bold; line-height: 1.4; cursor: text; }
h1:hover a.anchor, h2:hover a.anchor, h3:hover a.anchor, h4:hover a.anchor, h5:hover a.anchor, h6:hover a.anchor { text-decoration: none; }
h1 tt, h1 code { font-size: inherit; }
h2 tt, h2 code { font-size: inherit; }
h3 tt, h3 code { font-size: inherit; }
h4 tt, h4 code { font-size: inherit; }
h5 tt, h5 code { font-size: inherit; }
h6 tt, h6 code { font-size: inherit; }
h1 { padding-bottom: 0.3em; font-size: 2.25em; line-height: 1.2; border-bottom: 1px solid rgb(238, 238, 238); }
h2 { padding-bottom: 0.3em; font-size: 1.75em; line-height: 1.225; border-bottom: 1px solid rgb(238, 238, 238); }
h3 { font-size: 1.5em; line-height: 1.43; }
h4 { font-size: 1.25em; }
h5 { font-size: 1em; }
h6 { font-size: 1em; color: rgb(119, 119, 119); }
p, blockquote, ul, ol, dl, table { margin: 0.8em 0px; }
li > ol, li > ul { margin: 0px; }
hr { height: 2px; padding: 0px; margin: 16px 0px; background-color: rgb(231, 231, 231); border: 0px none; overflow: hidden; box-sizing: content-box; }
li p.first { display: inline-block; }
ul, ol { padding-left: 30px; }
ul:first-child, ol:first-child { margin-top: 0px; }
ul:last-child, ol:last-child { margin-bottom: 0px; }
blockquote { border-left: 4px solid rgb(223, 226, 229); padding: 0px 15px; color: rgb(119, 119, 119); }
blockquote blockquote { padding-right: 0px; }
table { padding: 0px; word-break: initial; }
table tr { border-top: 1px solid rgb(223, 226, 229); margin: 0px; padding: 0px; }
table tr:nth-child(2n), thead { background-color: rgb(248, 248, 248); }
table tr th { font-weight: bold; border-width: 1px 1px 0px; border-top-style: solid; border-right-style: solid; border-left-style: solid; border-top-color: rgb(223, 226, 229); border-right-color: rgb(223, 226, 229); border-left-color: rgb(223, 226, 229); border-image: initial; border-bottom-style: initial; border-bottom-color: initial; margin: 0px; padding: 6px 13px; }
table tr td { border: 1px solid rgb(223, 226, 229); margin: 0px; padding: 6px 13px; }
table tr th:first-child, table tr td:first-child { margin-top: 0px; }
table tr th:last-child, table tr td:last-child { margin-bottom: 0px; }
.CodeMirror-lines { padding-left: 4px; }
.code-tooltip { box-shadow: rgba(0, 28, 36, 0.3) 0px 1px 1px 0px; border-top: 1px solid rgb(238, 242, 242); }
.md-fences, code, tt { border: 1px solid rgb(231, 234, 237); background-color: rgb(248, 248, 248); border-radius: 3px; padding: 2px 4px 0px; font-size: 0.9em; }
code { background-color: rgb(243, 244, 244); padding: 0px 2px; }
.md-fences { margin-bottom: 15px; margin-top: 15px; padding-top: 8px; padding-bottom: 6px; }
.md-task-list-item > input { margin-left: -1.3em; }
@media print {
  html { font-size: 13px; }
  table, pre { break-inside: avoid; }
  pre { overflow-wrap: break-word; }
}
.md-fences { background-color: rgb(248, 248, 248); }
#write pre.md-meta-block { padding: 1rem; font-size: 85%; line-height: 1.45; background-color: rgb(247, 247, 247); border: 0px; border-radius: 3px; color: rgb(119, 119, 119); margin-top: 0px !important; }
.mathjax-block > .code-tooltip { bottom: 0.375rem; }
.md-mathjax-midline { background: rgb(250, 250, 250); }
#write > h3.md-focus::before { left: -1.5625rem; top: 0.375rem; }
#write > h4.md-focus::before { left: -1.5625rem; top: 0.285714rem; }
#write > h5.md-focus::before { left: -1.5625rem; top: 0.285714rem; }
#write > h6.md-focus::before { left: -1.5625rem; top: 0.285714rem; }
.md-image > .md-meta { border-radius: 3px; padding: 2px 0px 0px 4px; font-size: 0.9em; color: inherit; }
.md-tag { color: rgb(167, 167, 167); opacity: 1; }
.md-toc { margin-top: 20px; padding-bottom: 20px; }
.sidebar-tabs { border-bottom: none; }
#typora-quick-open { border: 1px solid rgb(221, 221, 221); background-color: rgb(248, 248, 248); }
#typora-quick-open-item { background-color: rgb(250, 250, 250); border-color: rgb(254, 254, 254) rgb(229, 229, 229) rgb(229, 229, 229) rgb(238, 238, 238); border-style: solid; border-width: 1px; }
.on-focus-mode blockquote { border-left-color: rgba(85, 85, 85, 0.12); }
header, .context-menu, .megamenu-content, footer { font-family: "Segoe UI", Arial, sans-serif; }
.file-node-content:hover .file-node-icon, .file-node-content:hover .file-node-open-state { visibility: visible; }
.mac-seamless-mode #typora-sidebar { background-color: var(--side-bar-bg-color); }
.md-lang { color: rgb(180, 101, 77); }
.html-for-mac .context-menu { --item-hover-bg-color: #E6F0FE; }
#md-notification .btn { border: 0px; }
.dropdown-menu .divider { border-color: rgb(229, 229, 229); }
.ty-preferences .window-content { background-color: rgb(250, 250, 250); }
.ty-preferences .nav-group-item.active { color: white; background: rgb(153, 153, 153); }

 .typora-export li, .typora-export p, .typora-export,  .footnote-line {white-space: normal;} 
</style>
</head>
<body class='typora-export os-windows' >
<div  id='write'  class = 'is-node'><p>&nbsp;</p><p>&nbsp;</p><p align="center">
    <font size="6"><b>CAPS Notes - Lecture 1 </b></font><br>
    <font size="6"><b>Learning and Decision-Making </b></font><br>
    <br>
    Leo Klenner, Henry Fung, Cory Combs<br>
    <br>
    Last updated: 10/24/2019
</p><br><br><br><br><br><br><br><br><br><br><br><br><br><br><h2><a name="overview" class="md-header-anchor"></a><span>Overview</span></h2><p><span>This course is about how machines - from single algorithms to autonomous agents - make decisions. The course explores how generalists, working together with specialists, can guide decision-making to ensure that the decisions machines make are &quot;good&quot;.</span></p><p><span>What does it mean to make a &quot;good&quot; decision? That&#39;s one of the themes we begin to unpack in this lecture.</span></p><p><span>Lecture one is a primer on algorithmic decision-making:</span></p><ul><li><span>What is an algorithm?</span></li><li><span>How do algorithms that train on data (i.e. learn) differ from those that don&#39;t train on data?</span></li></ul><p><span>We also explore, at a foundational level, how the decision-making of algorithms compares to that of humans:</span></p><ul><li><span>How can we conceptualize the different ways in which algorithms and humans make decisions? </span></li><li><span>How can we use an understanding of such differences to ensure that deploying an algorithm results in &quot;good&quot; decisions?</span></li></ul><br><h2><a name="1-definitions-and-background-of-ai" class="md-header-anchor"></a><span>1. Definitions and Background of AI</span></h2><h3><a name="definitions" class="md-header-anchor"></a><span>Definitions</span></h3><h4><a name="ai" class="md-header-anchor"></a><span>AI</span></h4><p><span>To talk about AI, we first need to define it. Based on [1, 2], we can outline four possible definitions of AI, all of the form &quot;AI is the field that aims to build...&quot;  The definitions can be read in two-dimensions. One dimension is whether the goal is to match human performance or ideal rationality. The other dimension is whether the goal is to build systems that reason or systems that act (without reason). </span></p><figure><table><thead><tr><th>&nbsp;</th><th><span>Human-based</span></th><th><span>Ideal Rationality-based</span></th></tr></thead><tbody><tr><td><strong><span>Reasoning-based</span></strong></td><td><span>Systems that think like humans</span></td><td><span>Systems that think rationally</span></td></tr><tr><td><strong><span>Behavior-based</span></strong></td><td><span>Systems that act like humans</span></td><td><span>Systems that act rationally</span></td></tr></tbody></table></figure><p align="center">
    <i>Table 1:	Overview of four definitions of AI
</i></p><p><span>Each definition reflects an approach within AI research and each comes with its own conceptual challenges. </span></p><p><span>In this course, we follow [2] and define AI as &quot;systems that act rationally&quot;. </span></p><p><span>Adopting this definition brings about a fundamental challenge concerning the type of rationality entailed in &quot;systems that act rationally&quot;. If this definition of AI forces us to interpret &quot;rationally&quot; as &quot;perfectly rational&quot;, then we run into a host of issues, from technical ones, such as how we can make sense of a system&#39;s decisions if they don&#39;t appear to be goal-directed, to ethical ones, such as whether each decision of the system must be seen as &quot;good&quot;.</span></p><p><span>To clarify this aspect, we need to understand what different types of rationality we can pursue with AI. </span></p><h4><a name="rationality" class="md-header-anchor"></a><span>Rationality</span></h4><p><span>Adopting the definitions of [3], we can differentiate between three types of rationality:</span></p><ul><li><strong><span>Perfect rationality</span></strong><span>, or the capacity to generate maximally successful behavior given the available information</span></li><li><strong><span>Calculative rationality</span></strong><span>, or the capacity to compute, in principle, the perfectly rational decision given the initially available information</span></li><li><strong><span>Bounded optimality</span></strong><span>, or the capacity to generate maximally successful behavior given the available information and computational resources</span></li></ul><figure><table><thead><tr><th><span>Type</span></th><th><span>Information</span></th><th><span>Computation</span></th><th><span>Time</span></th><th><span>Frequency</span></th><th><span>Desirability</span></th></tr></thead><tbody><tr><td><span>Perfect rationality</span></td><td><span>Yes</span></td><td><span>No</span></td><td><span>Yes</span></td><td><span>Rarely exists</span></td><td><span>High</span></td></tr><tr><td><span>Calculative rationality</span></td><td><span>Yes</span></td><td><span>Yes</span></td><td><span>No</span></td><td><span>Often exists</span></td><td><span>Low</span></td></tr><tr><td><span>Bounded optimality</span></td><td><span>Yes</span></td><td><span>Yes</span></td><td><span>Yes</span></td><td><span>Often exists</span></td><td><span>Depends on the bounds</span></td></tr></tbody></table></figure><p align="center">
    <i>Table 2: Overview of differences between types of rationality
</i></p><p><span>Table 2, above, further differentiates the types of rationality based on five variables:</span></p><ul><li><span>constraints on information</span></li><li><span>constraints on computation (either compute power available or program specification)</span></li><li><span>constraints on time (deadlines for making a decision)</span></li><li><span>frequency (how often the type of rationality can be observed for real-world systems)</span></li><li><span>desirability (how desirable it is to have a system of this type of rationality)</span></li></ul><p><span>To understand why calculative rationality might not be desirable, consider a chess program that choses the &quot;right&quot; move but takes 10</span><sup><span>5</span></sup><span> times too long to make it.</span></p><p><span>How does this distinction help us adopt the definition of &quot;systems that act rationally&quot;? When we talk about AI, we assume that a system&#39;s decisions are subject to bounded rationality. </span></p><h4><a name="agent-and-environment" class="md-header-anchor"></a><span>Agent and Environment</span></h4><p><span>Taking on the definition of bounded rationality, we know that the decisions we analyze are subject to two set of constraints:</span></p><ul><li><span>The constraints of the </span><strong><span>agent</span></strong><span> that makes the decisions, arising from how the agent is specified in terms of the algorithm that it computes and the resources it has to perform this computation.</span></li><li><span>The constraints of the </span><strong><span>environment</span></strong><span> in which the decisions are carried out, arising from aspects like imperfect information or other domain-specific features that might challenge the agent&#39;s performance (e.g. through necessitating fast decisions).</span></li></ul><p><span>There is also a third sets of constraints: </span></p><ul><li><span>The constraints of the </span><strong><span>agent-environment interaction</span></strong><span>, arising from aspects like whether the environment can return the feedback to the agent that the agent needs to make decisions. </span></li></ul><p><span>We will look more at agent-environment interaction later in the course, particularly in context of reinforcement learning. For now, it is important to realize that, while engineers might know a lot about the specification of the agent, robust decision-making also requires domain experts who can understand the environment in which the agent&#39;s decisions take place.</span></p><h3><a name="background" class="md-header-anchor"></a><span>Background</span></h3><p><span>When we talk about AI, we do not necessarily mean algorithms that train on data (i.e. learn). We could also be talking about </span><strong><span>rule-based systems</span></strong><span>, which make decisions by following sets of rules specified by their developers. These systems &quot;know&quot; without having &quot;learned&quot; through observations (i.e. data). </span></p><p><span>Systems that are based on </span><strong><span>machine learning</span></strong><span> techniques are a more recent development within AI. Note that both options are in line with the definition of &quot;systems that act rationally&quot;.</span></p><p><span>The central difference between the two options is that the decision-making of rule-based systems is </span><strong><span>deterministic</span></strong><span>, whereas the decision-making of learning-based systems is </span><strong><span>non-deterministic</span></strong><span>. (When we turn to neural networks later in the course, we will see that the </span><em><span>training</span></em><span> of a neural network is non-deterministic, whereas </span><em><span>once trained</span></em><span>, a neural network can be deterministic.) </span></p><p><span>&quot;Deterministic&quot; here means that given the same input, the system will always return the same output. With deterministic systems, we have a </span><strong><span>linear mapping</span></strong><span> of inputs to outputs.</span></p><pre spellcheck="false" class="md-fences md-end-block ty-contain-cm modeLoaded" lang=""><div class="CodeMirror cm-s-inner CodeMirror-wrap" lang=""><div style="overflow: hidden; position: relative; width: 3px; height: 0px; top: 0px; left: 7.59378px;"><textarea autocorrect="off" autocapitalize="off" spellcheck="false" tabindex="0" style="position: absolute; bottom: -1em; padding: 0px; width: 1000px; height: 1em; outline: none;"></textarea></div><div class="CodeMirror-scrollbar-filler" cm-not-content="true"></div><div class="CodeMirror-gutter-filler" cm-not-content="true"></div><div class="CodeMirror-scroll" tabindex="-1"><div class="CodeMirror-sizer" style="margin-left: 0px; margin-bottom: 0px; border-right-width: 0px; padding-right: 0px; padding-bottom: 0px;"><div style="position: relative; top: 0px;"><div class="CodeMirror-lines" role="presentation"><div role="presentation" style="position: relative; outline: none;"><div class="CodeMirror-measure"><span><span>​</span>x</span></div><div class="CodeMirror-measure"></div><div style="position: relative; z-index: 1;"></div><div class="CodeMirror-code" role="presentation" style=""><div class="CodeMirror-activeline" style="position: relative;"><div class="CodeMirror-activeline-background CodeMirror-linebackground"></div><div class="CodeMirror-gutter-background CodeMirror-activeline-gutter" style="left: 0px; width: 0px;"></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"># Simple example of deterministic decision-making:</span></pre></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"># whenever A is true, B is assigned the value 2</span></pre><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"><span cm-text="">​</span></span></pre><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;">if condition A is True:</span></pre><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"><span class="cm-tab" role="presentation" cm-text="	">    </span>then set variable B = 2</span></pre></div></div></div></div></div><div style="position: absolute; height: 0px; width: 1px; border-bottom: 0px solid transparent; top: 114px;"></div><div class="CodeMirror-gutters" style="display: none; height: 114px;"></div></div></div></pre><p><span>Note, that this doesn&#39;t mean that rule-based systems are necessarily simple. In fact, they can be extremely </span><strong><span>complicated</span></strong><span> - for example, they might contain multiple branches to account for alternative courses of action. However, unlike machine learning systems, rule-based systems are not </span><strong><span>complex</span></strong><span>: they are never greater than the sum of their parts.</span></p><p><span>&quot;Non-deterministic&quot; means that given the same input, the system will </span><em><span>not</span></em><span> always return the same output. With non-deterministic systems, we have a </span><strong><span>non-linear mapping</span></strong><span> of inputs to outputs. </span></p><pre spellcheck="false" class="md-fences md-end-block ty-contain-cm modeLoaded" lang=""><div class="CodeMirror cm-s-inner CodeMirror-wrap" lang=""><div style="overflow: hidden; position: relative; width: 3px; height: 0px; top: 0px; left: 7.59378px;"><textarea autocorrect="off" autocapitalize="off" spellcheck="false" tabindex="0" style="position: absolute; bottom: -1em; padding: 0px; width: 1000px; height: 1em; outline: none;"></textarea></div><div class="CodeMirror-scrollbar-filler" cm-not-content="true"></div><div class="CodeMirror-gutter-filler" cm-not-content="true"></div><div class="CodeMirror-scroll" tabindex="-1"><div class="CodeMirror-sizer" style="margin-left: 0px; margin-bottom: 0px; border-right-width: 0px; padding-right: 0px; padding-bottom: 0px;"><div style="position: relative; top: 0px;"><div class="CodeMirror-lines" role="presentation"><div role="presentation" style="position: relative; outline: none;"><div class="CodeMirror-measure"><pre><span>xxxxxxxxxx</span></pre></div><div class="CodeMirror-measure"></div><div style="position: relative; z-index: 1;"></div><div class="CodeMirror-code" role="presentation" style=""><div class="CodeMirror-activeline" style="position: relative;"><div class="CodeMirror-activeline-background CodeMirror-linebackground"></div><div class="CodeMirror-gutter-background CodeMirror-activeline-gutter" style="left: 0px; width: 0px;"></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"># Simple example of non-deterministic decision-making:</span></pre></div><div class="" style="position: relative;"><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"># whenever A is true, B is assigned one of the possible even values between 0</span></pre></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"># and n </span></pre><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"><span cm-text="">​</span></span></pre><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;">if condition A is True:</span></pre><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"><span class="cm-tab" role="presentation" cm-text="	">    </span>then set variable B = (choose random even number from range (0, n))</span></pre></div></div></div></div></div><div style="position: absolute; height: 0px; width: 1px; border-bottom: 0px solid transparent; top: 137px;"></div><div class="CodeMirror-gutters" style="display: none; height: 137px;"></div></div></div></pre><p><span>The period from 1987-1994 is often termed an &quot;AI winter&quot;, in which the evident limitations of deterministic &quot;if-then&quot; systems caused a slowdown in progress. In the late 1990s, progress picked up, and by the late 2000s massive advancements were achieved due both to the development of new training algorithms and the increasing availability of extremely large datasets on which to train. Non-deterministic systems were at the center of the deep learning revolution. We will discuss some of these algorithms and related developments later on.</span></p><p><span>The non-deterministic nature of machine learning leads, however, to a number of problems for implementing this technology in practice. Among others, these problems concern </span><strong><span>transparency</span></strong><span>, </span><strong><span>bias</span></strong><span>, </span><strong><span>predictability</span></strong><span>, and </span><strong><span>safety</span></strong><span>. We will address these problems as key themes throughout the course.</span></p><br><h2><a name="2-applications-and-risks-of-ai" class="md-header-anchor"></a><span>2. Applications and Risks of AI</span></h2><h3><a name="applications" class="md-header-anchor"></a><span>Applications</span></h3><p><span>AI has applications across many domains, from games to financial markets to strategy. Here, we briefly review applications of AI in financial markets, historically one of the richest domains of AI implementation.</span></p><h4><a name="ai-in-financial-markets" class="md-header-anchor"></a><span>AI in Financial Markets</span></h4><p><span>Here is a short timeline of AI in financial markets:</span></p><ul><li><span>1970s: rule-based algorithms start to execute trades</span></li><li><span>1975: creation of the first index fund, using rules to track the components of financial market indices</span></li><li><span>1990s: creation of exchange-traded funds (ETFs), which use rules to automate specific investment strategies, and of quantitative funds, which drive the use of advanced algorithms in financial markets</span></li><li><span>2010s: increasing focus on machine learning within quantitative funds, leading some funds to switch from hypothesis-driven to data-driven investment approaches</span></li></ul><p><span>Within this brief history, we can differentiate between four different types of agency in financial markets:</span></p><figure><table><thead><tr><th><span>Type</span></th><th><span>Primary function</span></th><th><span>Primary product</span></th></tr></thead><tbody><tr><td><span>Human</span></td><td><span>Create strategies</span></td><td><span>Mutual funds</span></td></tr><tr><td><span>Rules-based</span></td><td><span>Execute trades, or mimic human strategies and execute trades</span></td><td><span>Index funds</span></td></tr><tr><td><span>Human plus algorithm</span></td><td><span>Algorithms perform data analysis, humans select trades</span></td><td><span>Quant funds</span></td></tr><tr><td><span>Machine learning</span></td><td><span>Create and execute strategies</span></td><td><span>Quant funds</span></td></tr></tbody></table></figure><p align="center">
    <i>Table 3:	Overview of differences types of agency in financial markets
</i></p><p><span>Each type of agency comes with its own patterns of decision-making; for example, rule-based automated execution of trades is faster than human execution but bound to human-identified processes, and machine learning strategies may outperform some humans but lead to the creation of strategies that are opaque to humans. We will return to the associated risks later on. First, we explore the impact of algorithms on financial markets.</span></p><p><span>To understand the impact of AI on financial markets, we can, as a proxy, look at the allocation of U.S. public equity assets based on 2019 data:</span></p><p align="center">
    <img src="https://capsseminar.github.io/static/notes/lecture1/data1.png" alt="graphics" width="533" height="200"><br>
    <i>Figure 1: % of U.S. Public Equity Asset Holdings worth USD 31 trillion, adapted from [4]</i>
</p><p><span>Figure 1 shows that with 35.1% percent of holdings, automated funds are the dominant management solution within the U.S. public equity market, exceeding the holdings of human-managed funds by more than 10%.</span></p><p><span>Next, we want to understand the balance between rule-based strategies and machine learning strategies within automated funds:</span></p><p align="center">
    <img src="https://capsseminar.github.io/static/notes/lecture1/data2.png" alt="graphics" width="533" height="200"><br>
    <i>Figure 2: % of U.S. Public Equity Assets in different automated funds, adopted from [4]</i>
</p><p>&nbsp;</p><p><span>Figure 2 shows that of the total U.S. public equity asset holdings, only 2.4% are in quant funds, so in principle only a maximum of 2.4% would have exposure to machine learning-based decision-making. Given the interconnectedness of financial markets, this means that risks related to machine learning still matter. However, we also need to consider risks that come from rule-based decision-making.</span></p><h3><a name="risks" class="md-header-anchor"></a><span>Risks</span></h3><p><span>The risks associated with AI are manifold. A separate field, called AI safety [5, 6, 7], is dedicated to managing these risks. Here, we look at a small subset of risks, on which we will expand throughout the course. </span></p><h4><a name="machine-learning-specific-risks" class="md-header-anchor"></a><span>Machine Learning-Specific Risks</span></h4><p><span>Note that all of these risks are human-facing. </span></p><ul><li><p><strong><span>Overconfidence and mistrust:</span></strong><span> Results discovered through machine learning might be spurious [8], meaning that they are based on an artificial correlation between unrelated covariates, especially in high-dimensional datasets. It can take time for the spuriousness to become apparent, or it could even go undetected if not properly audited; hence, there is a risk of initial human overconfidence when working with relevant algorithms. At the same time, there might be persistent mistrust in such algorithms among key stakeholders even if results turn out to be non-spurious. This is often where transparency and explainability become paramount concerns.</span></p></li><li><p><strong><span>Lack of data oversight: </span></strong><span> Large datasets are required to train machine learning algorithms, but simply &quot;having big data&quot; isn&#39;t enough. There are numerous risks associated with lack of human data oversight. First, even if datasets appear reasonably large enough, their size might not truly be sufficient to completely train an algorithm, which can result in suboptimal performance. This is particularly the case if a dataset contains many observations of one feature or variable but very few of another, e.g. normal credit card activity vs. suspicious credit card activity. Second, training an algorithm on a dataset that is too large for the algorithm might result in the algorithm overfitting, i.e. learning oversimplified behavior that will not map to new, real-word data. Third, poorly selected datasets can lead to bias and &quot;garbage in, garbage out&quot; scenarios, such as when a system is designed to simply use the &quot;best&quot; correlation among uncorrelated features. Fourth, adversarial examples [9], i.e. data known and selected to distort system outputs, may be injected into the training data, leading to corrupted performance of the algorithm. This is a particular challenge in online natural language processing systems, including in chatbots.</span></p></li><li><p><strong><span>Lack of continuous adaptation:</span></strong><span> Environments like financial markets are in constant flux and humans must constantly keep learning to succeed across these changes. Once an algorithm has been trained and deployed, it might not take long before the training dataset has become outdated and the algorithms performance degraded. On the other hand, allowing the algorithm to learn continously throughout its deployment as well brings back the challenges of human oversight over the training data, compounded with the difficulty of management now being real-time rather than in a test environment.</span></p></li><li><p><strong><span>Lack of transparency and explainability:</span></strong><span> Advanced machine learning algorithms perform complex optimization processes to achieve their objectives. These processes are often opaque to humans and don&#39;t come with meaningful explanations attached. Although the algorithm might make the &quot;right&quot; decision, the humans who are working alongside the algorithm might not be able to make sense out of this decision, and consequently might remain incapable of developing appropriate responses on their end. This can lead to a joint performance failure.</span></p><p><span>A prominent example of a lack of transparency and explainability comes from the world of the strategy game Go. In match 2 against Lee Sedol, the 9 dan-ranked Go player (one of the world&#39;s top-ranked), DeepMind&#39;s machine learning-based Go-bot AlphaGo made its famous move 37 that baffled professional commentators, forced Lee to leave the room, and secured AlphaGo its second win. </span></p><p><span>The live-feed from the commentators [10] provides detailed insights into how humans might struggle to make sense of a decision made by a machine learning algorithm:</span></p><blockquote><p><span>That&#39;s a very surprising move. I thought that was a mistake. I thought it was click miss. Exactly, if we were in online Go, we would call it a &#39;clicko&#39;. </span></p></blockquote><p align="center">
    <img src="https://capsseminar.github.io/static/notes/lecture1/move_37_edit.png" alt="graphics" width="179" height="179"> <br>
    <i>Figure 3: In Match 2 against Lee Sedol (white), AlphaGo (black) makes its game-changing move 37</i>
</p><p><span>First, the commentators are convinced that AlphaGo&#39;s move must have been a mistake or technical error. Being unable to make sense out of the decision, they start to doubt their environment, which brings back the challenges of mistrust mentioned earlier. </span></p><p><span>Next, the commentators compare the decision to the alternatives they considered and understand. </span></p><blockquote><p><span>Yeah, it&#39;s a very strange move. Something like this [changes black&#39;s position on the board] would be a more normal move and then this [moves white&#39;s position on the board] is how white would respond. </span></p></blockquote><p><span>Finally, after seeing the ambigious response of another human impacted by this decision, the commentators resign themselves to making a hedged statement and emphasizing that they would need more time and information about how the game will proceed to make a more definite statement.</span></p><blockquote><p><span>Lee has left the room. He left the room after this move.  Just to recover from this move. It&#39;s a very surprising move. I don&#39;t know whether it&#39;s a good or a bad move at this point. </span></p></blockquote><p><span>How can we manage these risks? From managing overconfidence to managing a lack of transparency, we have seen that building a response to risks associated with machine-learning takes time. However, in real-time environments, time is often highly constrained. </span></p></li></ul><h4><a name="ecosystem-specific-risks" class="md-header-anchor"></a><span>Ecosystem-Specific Risks</span></h4><p><span>In this section, we briefly look at a risk associated with rule-based decision-making: the fast and automated execution of decisions. </span></p><p><span>The decision-making of machine learners might take place on top of an infrastructure of fast rule-based execution, eliminating the time humans would need to meaningfully respond to challenges like mitigating the opacity of a move 37. This makes fast rule-based execution, paired with human stakeholders&#39; comparatively slow response time and heterogenous information, an ecosystem-specific risk for machine learning-based decision-making. </span></p><ul><li><p><strong><span>Complexity and speed</span></strong><span>: Software development processes are complex. They often involve multiple versions of a program existing in the same codebase. The goal of these processes is often speed. Engineers build a program that, among other things, works faster than other programs. The risks from development complexity and speed of execution came together in a 2012 stock trading disruption at Knight Capital Group that caused the financial services firm to lose 70.60% of its market value of USD 1.5bn over two days for an error that was discovered and fixed </span><em><span>in less than an hour</span></em><span>. </span></p><p align="center">
    <img src="https://capsseminar.github.io/static/notes/lecture1/knight3.png" alt="graphics" width="498," height="293"> <br>
    <i>Figure 4: Initial drop in the stock price of Knight Capital Group following a trading disruption on August 1, 2012</i>
</p><p><span>A detailed account of this event of how Knight enabled the fatal trading disruption through careless changes to its software is given in [11]. Here, we represent the changes to Knight&#39;s case in four steps across Figures 5 and 6. Blue lines represent active connections between the components of Knight&#39;s trading program, grey lines represent inactive connections. </span></p><p align="center">
    <img src="https://capsseminar.github.io/static/Graphic%20Card%20set.png" alt="graphics"> <br>
    <i>Figure 5: Step 1 and 2 of the changes to Knight's codebase </i>
</p><ul><li><p><strong><span>Step 1:</span></strong><span> Knight has a functioning trading system that receives orders, processes their execution, controls this execution, and sends the processed orders to markets. Through development bad practice, an old algorithm built only for experimental purposes remains part of the codebase of the trading system, although it cannot be activated under the current system specification. The algorithm is designed to </span><em><span>buy high</span></em><span> and </span><em><span>sell low</span></em><span>. Although the algorithm currently sits as &quot;dead code&quot; in the codebase, it retains the functionality to send orders to markets. </span></p></li><li><p><strong><span>Step 2:</span></strong><span> Knight makes updates to the control component in its codebase. These updates bring improvements but through further development bad practice detach the control from the experimental algorithm. </span></p><p align="center">
    <img src="https://capsseminar.github.io/static/notes/lecture1/Slide2.PNG" alt="graphics"> <br>
    <i>Figure 6: Steps 3 and 4 of the changes to Knight's codebase </i>
</p></li><li><p><strong><span>Step 3:</span></strong><span> To make its system compatible with a new market, Knight builds a new execution component for its system. These changes are made under severe time pressure and introduce a critical development flaw. The code for the new execution component repurposes the activation code of the experimental algorithm. As a result, the experimental algorithm can now receive orders. It does not yet receive these orders because the new execution system is still higher up in the hierarchy of the program, so all the input signals are fed into the execution component. </span></p></li><li><p><strong><span>Step 4:</span></strong><span> The updates to Knight&#39;s system go live on eight servers and Knight starts receiving orders. However, under time pressure, one of Knight&#39;s developers has forgotten to update one of the eight servers correctly. Hence, this server does not have access to the new execution component. On this server, the input signal, i.e. the orders that Knight receives from its clients, are now fed into the experimental algorithm, which starts to </span><em><span>buy high</span></em><span> and </span><em><span>sell low</span></em><span> without being checked by any controls. After less than an hour, developers locate the error and shut the server down. However, they made the fatal flaw of reverting to the old code on all eight servers and take the complete system live again. As it turned out, the experimental algorithm was now </span><em><span>activated on all eight servers</span></em><span>. The cumulative losses from the trades were irreversible and ended up reducing Knight&#39;s stock price by 70%.</span></p></li></ul><p><span>The story of Knight capital shows the multiple risks that arise from different patterns of decision-making - in this case, the rapid execution of an algorithm paired with the adaptable but sometimes inconsistent decision-making of human developers. In the next section, we&#39;ll look more at human decision-making from the perspective of human situational learning.</span></p><p><span>As a last note, reading through this section and looking at Figures 5 and 6, you have familiarized yourself with two important concepts, those of an algorithm and a program:</span></p><ul><li><span>An </span><strong><span>algorithm</span></strong><span> is &quot;a group of components talking to each other in sequence&quot;, where each component can be a simple instruction like &quot;when A is True, then do B&quot; or a more sophisticated function like &quot;do convex optimization for A&quot;.</span></li><li><span>A </span><strong><span>program</span></strong><span> is also &quot;a group of components talking to each other in sequence&quot;, but where each component is an algorithm.</span></li></ul></li></ul><br><h2><a name="3-human-learning-and-decision-making" class="md-header-anchor"></a><span>3. Human Learning and Decision-Making</span></h2><p><span>We&#39;ll now discuss the case study for this session, &quot;</span><a href='https://capsseminar.github.io/static/Case1.pdf'><span>Learning in a Counterinsurgency Team</span></a><span>&quot;.</span></p><br><p><strong><span>Jason Amerine</span></strong><span>:</span></p><ul><li><p><em><span>Learning from instruction:</span></em></p><ul><li><span>Amerine mentions the importance of instructors during training.</span></li></ul><blockquote><p><span>You get out of these courses and sometimes you have instructors that take what they teach very seriously and other times you don’t.</span></p></blockquote></li><li><p><em><span>Rules and applying them to new environments:</span></em></p><ul><li><span>Amerine mentions that the transition from training to deployment is linear.</span></li></ul><blockquote><p><span>I found was that every major lesson I have learned throughout my career, whether it was in the Q Course [Army Special Forces Qualification Course] or Ranger School, I mean, everything that I was taught in the school house, I applied over there.</span></p></blockquote><blockquote><p><span>All the major muscle movements during the campaign we really had been taught.</span></p></blockquote></li><li><p><em><span>Exception handling and continuous learning:</span></em></p><ul><li><span>Amerine mentions one core exception to the linear transition from training to deployment and how it was solved by generating new rules of behavior.</span></li></ul><blockquote><p><span>You had certain unique areas, when we talk about the Horse Soldiers ... I mean that was something you couldn’t have foreseen and literally was an act of God that we had the right officer there who could teach his people how to ride.</span></p></blockquote></li></ul><br><p><strong><span>Mark Nutsch</span></strong><span>:</span></p><ul><li><p><em><span>Rules and applying them to new environments:</span></em></p><ul><li><span>Nutsch mentions that the transition from training to deployment is linear but focuses on process over instructors.</span></li></ul><blockquote><p><span>But even in that new situation, the guys kept going, ‘Hey, we have been here before, remember Special Forces training, remember Robin’s Sage at this phase of insurgency, you know as that would progress, remember that.’</span></p></blockquote></li><li><p><em><span>Adaptation and learning how to learn:</span></em></p><ul><li><span>Nutsch describes training as learning how to learn, so the team can adapt fast to changes in the environment and its behavior reflects new information.</span></li></ul><blockquote><p><span>But the sergeants and I, coming back as we’re talking about this, we did the things you do in training. Each day we would do lessons learned, an internal AAR [After Action Report], whether it was five minutes or fifteen minutes, sit down and go ‘Damn, what nearly killed us today? How do we make sure that doesn’t happen again? You know, how do we survive the next hour? And how do we win?’</span></p></blockquote><blockquote><p><span>I would have to say, even in our mission, we were the students.</span></p></blockquote></li><li><p><em><span>Breadth of relevant training:</span></em></p><ul><li><span>Nutsch mentions that the relevant training for him extended across his entire life and career.</span></li></ul><blockquote><p><span>You relied on that training that you had, the leadership lessons, people, mentors, that talked to you, every aspect of my career up to that point, to include character building events I had as a teenager through high school and college, all of that came to that focal point in my life on that battlefield.</span></p></blockquote></li><li><p><span>Pattern matching and learning with imperfectly labeled data:</span></p><ul><li><span>Nutsch describes the process of matching patterns based on a different local information system.</span></li></ul><blockquote><p><span>They couldn’t read a map but they could describe to you passionately ‘It’s this village, don’t you understand? It’s this village right over here. It’s this guy, he’s the one we’re after.’</span></p></blockquote></li></ul><p><strong><span>Core themes:</span></strong></p><ul><li><span>Application of rules vs adaptation</span></li><li><span>Learning from a supervising instructor vs learning from interaction with the environment</span></li><li><span>Learning through guidance vs learning through trial-and-error</span></li><li><span>Transfer of knowledge across domains vs learning how to learn</span></li></ul><p><strong><span>Differences between Amerine and Nutsch:</span></strong></p><ul><li><span>Amerine emphasizes the value of having knowledge that generalizes across environments</span></li><li><span>Nutsch emphasizes the value of learning how to learn in training, so that even without knowledge about a new environment, fast adaptation is possible</span></li><li><span>Amerine emphasizes </span><em><span>instructor-led</span></em><span> learning</span></li><li><span>Nutsch emphasizes </span><em><span>process-driven</span></em><span> learning</span></li></ul><br><h2><a name="4-types-of-machine-learning" class="md-header-anchor"></a><span>4. Types of Machine Learning</span></h2><p><span>In this section, we give a very concise, high-level overview of the major types of machine learning, which we will explore in detail over the next three sessions.</span></p><ul><li><p><strong><span>Supervised learning</span></strong></p><ul><li><span>Supervised learning is learning from labeled data, where the labels are provided to the algorithm by a human and the algorithm learns to apply these labels to new data, e.g. for </span><em><span>prediction</span></em><span>.</span></li></ul></li><li><p><strong><span>Unsupervised learning</span></strong></p><ul><li><span>Unsupervised learning is learning from data that does not have labels, for the purpose of identifying features that can be used to group or cluster the data, e.g. for </span><em><span>pattern detection</span></em><span>.</span></li></ul></li><li><p><strong><span>Reinforcement learning</span></strong></p><ul><li><span>Reinforcement learning is about learning through trial and error from interaction with an environment what the best sequence of actions is to achieve a specified goal, e.g. for </span><em><span>autonomous control</span></em><span>.</span></li></ul></li><li><p><strong><span>Meta-learning and transfer learning</span></strong></p><ul><li><span>Meta- and transfer learning are at the forefront of current AI research and concern how algorithms can learn </span><em><span>how</span></em><span> to learn and quickly adapt previously learned behavior to new environments.</span></li></ul></li></ul><p><span>As we explore these learning types over the coming sessions, consider their roots in alternative modes of human learning - for example, those that Amerine and Nutsch discussed. Each type of machine learning was developed around a particular paradigm of how learning can occur, intentionally rooted in human experience. Just as with human learning, machine learning types can be coordinated within a given program or system to significantly bolster performance, leveraging strengths and covering weaknesses of algorithms of a given type. We will focus on each learning type in turn, but at the end of the course will return the spotlight to real-world deployment possibilities integrating these technologies for optimal effect.</span></p><br><h2><a name="5-learning-and-optimality" class="md-header-anchor"></a><span>5. Learning and Optimality</span></h2><p><span>If we search for a single unified expression of how each type of machine learning works, it would be mathematical optimization. Here, we give a brief overview of optimization in a mathematical sense and then focus on how optimization interacts with policy.</span></p><p><span>In simple mathematical terms, when we learn a task, our goal is to </span><strong><span>minimize the rate of errors</span></strong><span>. Equivalently, we can state that our goal is to </span><strong><span>maximize the rate of success</span></strong><span>.</span></p><p><span>Both minimizing error and maximizing success sound intuitive but there are several other elements we need to consider to understand optimization.</span></p><p><span>The main element is that we should understand minimization and maximization in a spatial sense. Optimization is a </span><strong><span>search</span></strong><span> across a continuous space (representing the space of possible decisions) for the minimum or maximum value in this space. For an example of such a </span><strong><span>search space</span></strong><span>, see Figure 7 below. </span></p><p align="center">
    <img src="https://capsseminar.github.io/static/notes/lecture1/surface.png" alt="graphics" width="280" ,="" height="210"> <br>
    <i>Figure 7: Search space of an optimization problem with local and global maxima </i>
</p><p><span>When we start to learn, we typically start at a random point in this space and then explore the space over time to find its respective minimum or maximum.</span></p><p><span>In this search, we need to differentiate between </span><strong><span>local optima</span></strong><span> and </span><strong><span>global optima</span></strong><span>. Local optima are optimal points that are optimal only for a subregion of the space. Global optima are optimal points for the </span><em><span>entire</span></em><span> considered space. Sometimes as we explore a space, the learner may get stuck in a local optimum, unable to tell whether a more optimal space exists. There are numerous mathematical tools to resolve this, including the addition of </span><strong><span>constraints</span></strong><span> that reduce the search space and facilitate the learner&#39;s search.</span></p><p><span>When we discuss constraints, we need to differentiate between two uses of the term &quot;constraint&quot;:</span></p><ul><li><span>Constraints that change </span><em><span>whether</span></em><span> the algorithm can solve the problem, e.g. funding, the computational resources available, and engineer availability. We can call these logistical or real-world constraints; they are typically beyond our direct control.</span></li><li><span>Constraints that change </span><em><span>how</span></em><span> the algorithm solves the problem, e.g. removing certain optimal or non-optimal decisions from the search space. We can call these mathematical constraints, understanding that these are imposed by the developers for a particular purpose; they are in the developers&#39; control.</span></li></ul><p><span>The usage is usually clear from context, but when bridging the gap between policy makers and engineers, it is important to be sensitive to the distinction and clarify when two sides may mean different things. In our course discussions, when we say that we &quot;add constraints&quot;, we are talking about mathematical constraints that change </span><em><span>how</span></em><span> the problem gets solved.</span></p><p><span>Note, that both types of constraint bring aspects of policy and strategy into the calculus of optimization. The reason for this is simple:</span></p><ul><li><span>Making the &quot;right&quot; decision requires a consistent definition of what constitutes an error or success for a selected task. Both types of constraint can influence these definitions, in different manners.</span></li></ul><p><span>Why do we need to be consistent in stating what we mean by error or success? </span></p><p><span>We need to be consistent in defining what we want to optimize because if we don&#39;t guide the learner in a consistent way, </span><em><span>it will find all kinds of unexpected and undesired solutions</span></em><span> within the wide space that it searches. Some of these solutions may run counter to what we wanted the algorithm to learn in the first place.</span></p><p><span>We can describe the tradeoffs around the target specification for optimization with the following terms, as presented in [12]:</span></p><ul><li><strong><span>Ideal specification</span></strong><span> describes the &quot;wishes&quot; or intentions of the system designers, which correspond to the hypothetical description of an ideal system.</span></li><li><strong><span>Design specification</span></strong><span> describes the &quot;blueprint&quot; of the system, corresponding to the specifications that the designers </span><em><span>actually use</span></em><span> to build the system.</span></li><li><strong><span>Revealed specification</span></strong><span> describes the actual &quot;behavior&quot; exhibited by the system, i.e. </span><em><span>what actually happens</span></em></li></ul><p><span>To understand the challenges of consistent target specification, consider the following real-world example: </span></p><ul><li><span>In a computer game version of a boat race, multiple bonus packages that provide points and turbo boosts are placed across the racetrack from start to end. You want to train an algorithm that learns how to win the race. Maximization requires numerical values, which the points associated with the packages conveniently provide. Thus, to achieve the objective, you tell the algorithm &quot;maximize bonus points&quot;, assuming that this maximization will lead to the fastest traversal of the racetrack from start to end.</span></li></ul><p><span>Figure 8 shows what happened when researchers implemented the above logic for the game </span><em><span>CoastRunners</span></em><span>  [12]. (It is best viewed as an </span><a href='https://miro.medium.com/max/478/0*UoBrrtrY2rx2SvXr'><span>animation</span></a><span>.)</span></p><p align="center">
    <img src="https://capsseminar.github.io/static/notes/lecture1/boat4.png" alt="graphics" idth="199" height="149">
    <img src="https://capsseminar.github.io/static/notes/lecture1/boat3.png" alt="graphics" idth="199" height="149">
    <br>
<i>Figure 8: Faulty optimization in the CoastRunners game</i></p><p><span>Instead of finishing the race, the machine learner found a laguna in which &quot;turbo boosts&quot; are spawned in regular intervals. The learner discovers that circling through these boosts  - ad infinitum - yields a higher bonus score than simply following the racetrack from start to end. To time the respawn of the &quot;turbo boosts&quot;, the learner circles through the lagoon, in the process crashing into other boats and parts of the environment. The algorithm never completes the race. However, it does achieve a substantially higher score than the boats that finish the race. Thus, the revealed specification differs radically from the design and ideal specification of the algorithm.</span></p><p><span>How do we guard against (metaphorical) boats that never actually finish the race? To ensure that optimization is consistent with policy intentions and aligned with human values, both generalist and domain experts can provide invaluable guidance to the specialists who develop the relevant algorithms, especially in complex real-world environments. This cooperation, in turn, requires the capacity to translate between these stakeholders and across disciplinary lines.</span></p><p><span>What does all of the above tell us about &quot;good&quot; decisions? To start with, we can assert the following:</span></p><ul><li><span>While all &quot;good&quot; decisions are optimized, not all optimized decisions are &quot;good&quot;.</span></li></ul><p><span>&quot;Good&quot; is a joint product of policy, ethical, and technical considerations. The coming sessions introduce key technical possibilities and risks of various machine learning technologies, tying them to respective ethical and policy issues. But as new systems continually evolve, it is essential to continue exploring rising trends. These lectures are first steps toward learning how to learn about AI.</span></p><h3><a name="references" class="md-header-anchor"></a><span>References</span></h3><p><span>[1] Steward Russel and Peter Norvig. 2019. Artificial Intelligence: A Modern Approach. Available at: </span><a href='http://aima.cs.berkeley.edu/' target='_blank' class='url'>http://aima.cs.berkeley.edu/</a></p><p><span>[2] Selmer Bringsjord and Naveen Sundar Govindarajulu. 2018. Artificial Intelligence. Available at: </span><a href='https://plato.stanford.edu/entries/artificial-intelligence/' target='_blank' class='url'>https://plato.stanford.edu/entries/artificial-intelligence/</a></p><p><span>[3] Stewart Russel. 2015. Rationality and Intelligence: A Brief Update. Available at: </span><a href='https://people.eecs.berkeley.edu/~russell/papers/ptai13-intelligence.pdf' target='_blank' class='url'>https://people.eecs.berkeley.edu/~russell/papers/ptai13-intelligence.pdf</a></p><p><span>[4] The Economist. 2019. The stockmarket is now run by computers, algorithms and passive managers. Available at: </span><a href='https://www.economist.com/briefing/2019/10/05/the-stockmarket-is-now-run-by-computers-algorithms-and-passive-managers' target='_blank' class='url'>https://www.economist.com/briefing/2019/10/05/the-stockmarket-is-now-run-by-computers-algorithms-and-passive-managers</a></p><p><span>[5] Jan Leike et al. 2017. AI Safety Gridworlds. Available at: </span><a href='https://arxiv.org/abs/1711.09883' target='_blank' class='url'>https://arxiv.org/abs/1711.09883</a></p><p><span>[6] Paul Christiano and Greg Brockman. 2016. Concrete AI Safety Problems. Available at: </span><a href='https://openai.com/blog/concrete-ai-safety-problems/' target='_blank' class='url'>https://openai.com/blog/concrete-ai-safety-problems/</a></p><p><span>[7] DeepMind&#39;s AI Safety Team, see: </span><a href='https://deepmind.com/safety-and-ethics' target='_blank' class='url'>https://deepmind.com/safety-and-ethics</a></p><p><span>[8] Jianquing Fan and Wen-Xin Zhou. 2016. Guarding Against Spurious Discoveries in High Dimensions. Available at: </span><a href='http://www.jmlr.org/papers/volume17/16-068/16-068.pdf'><span>http://www.jmlr.org/papers/volume17/16-068/16-068.pdf</span></a></p><p><span>[9] Ian Goodfellow et al. 2017. Attacking Machine Learning with Adversarial Examples. Available at: </span><a href='https://openai.com/blog/adversarial-example-research/'><span>https://openai.com/blog/adversarial-example-research/</span></a></p><p><span>[10] Lee Sedol vs Alpha Go. 2016. Video available at: </span><a href='https://www.youtube.com/watch?v=JNrXgpSEEIE'><span>https://www.youtube.com/watch?v=JNrXgpSEEIE</span></a></p><p><span>[11] Bishr Tabbaa. 2018 The Rise and Fall of Knight Capital - Buy High, Sell Low. Rinse and Repeat. Available at: </span><a href='https://medium.com/dataseries/the-rise-and-fall-of-knight-capital-buy-high-sell-low-rinse-and-repeat-ae17fae780f6'><span>https://medium.com/dataseries/the-rise-and-fall-of-knight-capital-buy-high-sell-low-rinse-and-repeat-ae17fae780f6</span></a></p><p><span>[12] Pedro Ortega et al. 2018. Building safe artificial intelligence: specification, robustness and assurance. Available at: </span><a href='https://medium.com/@deepmindsafetyresearch/building-safe-artificial-intelligence-52f5f75058f1'><span>https://medium.com/@deepmindsafetyresearch/building-safe-artificial-intelligence-52f5f75058f1</span></a></p></div>
</body>
</html>
