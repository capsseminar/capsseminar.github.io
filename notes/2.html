<!doctype html>
<html>
<head>
<meta charset='UTF-8'><meta name='viewport' content='width=device-width initial-scale=1'>
<title>L2notes_new_order</title><link href='https://fonts.loli.net/css?family=Open+Sans:400italic,700italic,700,400&subset=latin,latin-ext' rel='stylesheet' type='text/css' /><style type='text/css'>html {overflow-x: initial !important;}:root { --bg-color:#ffffff; --text-color:#333333; --select-text-bg-color:#B5D6FC; --select-text-font-color:auto; --monospace:"Lucida Console",Consolas,"Courier",monospace; }
html { font-size: 14px; background-color: var(--bg-color); color: var(--text-color); font-family: "Helvetica Neue", Helvetica, Arial, sans-serif; -webkit-font-smoothing: antialiased; }
body { margin: 0px; padding: 0px; height: auto; bottom: 0px; top: 0px; left: 0px; right: 0px; font-size: 1rem; line-height: 1.42857; overflow-x: hidden; background: inherit; tab-size: 4; }
iframe { margin: auto; }
a.url { word-break: break-all; }
a:active, a:hover { outline: 0px; }
.in-text-selection, ::selection { text-shadow: none; background: var(--select-text-bg-color); color: var(--select-text-font-color); }
#write { margin: 0px auto; height: auto; width: inherit; word-break: normal; overflow-wrap: break-word; position: relative; white-space: normal; overflow-x: visible; padding-top: 40px; }
#write.first-line-indent p { text-indent: 2em; }
#write.first-line-indent li p, #write.first-line-indent p * { text-indent: 0px; }
#write.first-line-indent li { margin-left: 2em; }
.for-image #write { padding-left: 8px; padding-right: 8px; }
body.typora-export { padding-left: 30px; padding-right: 30px; }
.typora-export .footnote-line, .typora-export li, .typora-export p { white-space: pre-wrap; }
@media screen and (max-width: 500px) {
  body.typora-export { padding-left: 0px; padding-right: 0px; }
  #write { padding-left: 20px; padding-right: 20px; }
  .CodeMirror-sizer { margin-left: 0px !important; }
  .CodeMirror-gutters { display: none !important; }
}
#write li > figure:last-child { margin-bottom: 0.5rem; }
#write ol, #write ul { position: relative; }
img { max-width: 100%; vertical-align: middle; }
button, input, select, textarea { color: inherit; font: inherit; }
input[type="checkbox"], input[type="radio"] { line-height: normal; padding: 0px; }
*, ::after, ::before { box-sizing: border-box; }
#write h1, #write h2, #write h3, #write h4, #write h5, #write h6, #write p, #write pre { width: inherit; }
#write h1, #write h2, #write h3, #write h4, #write h5, #write h6, #write p { position: relative; }
p { line-height: inherit; }
h1, h2, h3, h4, h5, h6 { break-after: avoid-page; break-inside: avoid; orphans: 2; }
p { orphans: 4; }
h1 { font-size: 2rem; }
h2 { font-size: 1.8rem; }
h3 { font-size: 1.6rem; }
h4 { font-size: 1.4rem; }
h5 { font-size: 1.2rem; }
h6 { font-size: 1rem; }
.md-math-block, .md-rawblock, h1, h2, h3, h4, h5, h6, p { margin-top: 1rem; margin-bottom: 1rem; }
.hidden { display: none; }
.md-blockmeta { color: rgb(204, 204, 204); font-weight: 700; font-style: italic; }
a { cursor: pointer; }
sup.md-footnote { padding: 2px 4px; background-color: rgba(238, 238, 238, 0.7); color: rgb(85, 85, 85); border-radius: 4px; cursor: pointer; }
sup.md-footnote a, sup.md-footnote a:hover { color: inherit; text-transform: inherit; text-decoration: inherit; }
#write input[type="checkbox"] { cursor: pointer; width: inherit; height: inherit; }
figure { overflow-x: auto; margin: 1.2em 0px; max-width: calc(100% + 16px); padding: 0px; }
figure > table { margin: 0px !important; }
tr { break-inside: avoid; break-after: auto; }
thead { display: table-header-group; }
table { border-collapse: collapse; border-spacing: 0px; width: 100%; overflow: auto; break-inside: auto; text-align: left; }
table.md-table td { min-width: 32px; }
.CodeMirror-gutters { border-right: 0px; background-color: inherit; }
.CodeMirror-linenumber { user-select: none; }
.CodeMirror { text-align: left; }
.CodeMirror-placeholder { opacity: 0.3; }
.CodeMirror pre { padding: 0px 4px; }
.CodeMirror-lines { padding: 0px; }
div.hr:focus { cursor: none; }
#write pre { white-space: pre-wrap; }
#write.fences-no-line-wrapping pre { white-space: pre; }
#write pre.ty-contain-cm { white-space: normal; }
.CodeMirror-gutters { margin-right: 4px; }
.md-fences { font-size: 0.9rem; display: block; break-inside: avoid; text-align: left; overflow: visible; white-space: pre; background: inherit; position: relative !important; }
.md-diagram-panel { width: 100%; margin-top: 10px; text-align: center; padding-top: 0px; padding-bottom: 8px; overflow-x: auto; }
#write .md-fences.mock-cm { white-space: pre-wrap; }
.md-fences.md-fences-with-lineno { padding-left: 0px; }
#write.fences-no-line-wrapping .md-fences.mock-cm { white-space: pre; overflow-x: auto; }
.md-fences.mock-cm.md-fences-with-lineno { padding-left: 8px; }
.CodeMirror-line, twitterwidget { break-inside: avoid; }
.footnotes { opacity: 0.8; font-size: 0.9rem; margin-top: 1em; margin-bottom: 1em; }
.footnotes + .footnotes { margin-top: 0px; }
.md-reset { margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: top; background: 0px 0px; text-decoration: none; text-shadow: none; float: none; position: static; width: auto; height: auto; white-space: nowrap; cursor: inherit; -webkit-tap-highlight-color: transparent; line-height: normal; font-weight: 400; text-align: left; box-sizing: content-box; direction: ltr; }
li div { padding-top: 0px; }
blockquote { margin: 1rem 0px; }
li .mathjax-block, li p { margin: 0.5rem 0px; }
li { margin: 0px; position: relative; }
blockquote > :last-child { margin-bottom: 0px; }
blockquote > :first-child, li > :first-child { margin-top: 0px; }
.footnotes-area { color: rgb(136, 136, 136); margin-top: 0.714rem; padding-bottom: 0.143rem; white-space: normal; }
#write .footnote-line { white-space: pre-wrap; }
@media print {
  body, html { border: 1px solid transparent; height: 99%; break-after: avoid; break-before: avoid; }
  #write { margin-top: 0px; padding-top: 0px; border-color: transparent !important; }
  .typora-export * { -webkit-print-color-adjust: exact; }
  html.blink-to-pdf { font-size: 13px; }
  .typora-export #write { padding-left: 32px; padding-right: 32px; padding-bottom: 0px; break-after: avoid; }
  .typora-export #write::after { height: 0px; }
}
.footnote-line { margin-top: 0.714em; font-size: 0.7em; }
a img, img a { cursor: pointer; }
pre.md-meta-block { font-size: 0.8rem; min-height: 0.8rem; white-space: pre-wrap; background: rgb(204, 204, 204); display: block; overflow-x: hidden; }
p > .md-image:only-child:not(.md-img-error) img, p > img:only-child { display: block; margin: auto; }
p > .md-image:only-child { display: inline-block; width: 100%; }
#write .MathJax_Display { margin: 0.8em 0px 0px; }
.md-math-block { width: 100%; }
.md-math-block:not(:empty)::after { display: none; }
[contenteditable="true"]:active, [contenteditable="true"]:focus { outline: 0px; box-shadow: none; }
.md-task-list-item { position: relative; list-style-type: none; }
.task-list-item.md-task-list-item { padding-left: 0px; }
.md-task-list-item > input { position: absolute; top: 0px; left: 0px; margin-left: -1.2em; margin-top: calc(1em - 10px); border: none; }
.math { font-size: 1rem; }
.md-toc { min-height: 3.58rem; position: relative; font-size: 0.9rem; border-radius: 10px; }
.md-toc-content { position: relative; margin-left: 0px; }
.md-toc-content::after, .md-toc::after { display: none; }
.md-toc-item { display: block; color: rgb(65, 131, 196); }
.md-toc-item a { text-decoration: none; }
.md-toc-inner:hover { text-decoration: underline; }
.md-toc-inner { display: inline-block; cursor: pointer; }
.md-toc-h1 .md-toc-inner { margin-left: 0px; font-weight: 700; }
.md-toc-h2 .md-toc-inner { margin-left: 2em; }
.md-toc-h3 .md-toc-inner { margin-left: 4em; }
.md-toc-h4 .md-toc-inner { margin-left: 6em; }
.md-toc-h5 .md-toc-inner { margin-left: 8em; }
.md-toc-h6 .md-toc-inner { margin-left: 10em; }
@media screen and (max-width: 48em) {
  .md-toc-h3 .md-toc-inner { margin-left: 3.5em; }
  .md-toc-h4 .md-toc-inner { margin-left: 5em; }
  .md-toc-h5 .md-toc-inner { margin-left: 6.5em; }
  .md-toc-h6 .md-toc-inner { margin-left: 8em; }
}
a.md-toc-inner { font-size: inherit; font-style: inherit; font-weight: inherit; line-height: inherit; }
.footnote-line a:not(.reversefootnote) { color: inherit; }
.md-attr { display: none; }
.md-fn-count::after { content: "."; }
code, pre, samp, tt { font-family: var(--monospace); }
kbd { margin: 0px 0.1em; padding: 0.1em 0.6em; font-size: 0.8em; color: rgb(36, 39, 41); background: rgb(255, 255, 255); border: 1px solid rgb(173, 179, 185); border-radius: 3px; box-shadow: rgba(12, 13, 14, 0.2) 0px 1px 0px, rgb(255, 255, 255) 0px 0px 0px 2px inset; white-space: nowrap; vertical-align: middle; }
.md-comment { color: rgb(162, 127, 3); opacity: 0.8; font-family: var(--monospace); }
code { text-align: left; vertical-align: initial; }
a.md-print-anchor { white-space: pre !important; border-width: initial !important; border-style: none !important; border-color: initial !important; display: inline-block !important; position: absolute !important; width: 1px !important; right: 0px !important; outline: 0px !important; background: 0px 0px !important; text-decoration: initial !important; text-shadow: initial !important; }
.md-inline-math .MathJax_SVG .noError { display: none !important; }
.html-for-mac .inline-math-svg .MathJax_SVG { vertical-align: 0.2px; }
.md-math-block .MathJax_SVG_Display { text-align: center; margin: 0px; position: relative; text-indent: 0px; max-width: none; max-height: none; min-height: 0px; min-width: 100%; width: auto; overflow-y: hidden; display: block !important; }
.MathJax_SVG_Display, .md-inline-math .MathJax_SVG_Display { width: auto; margin: inherit; display: inline-block !important; }
.MathJax_SVG .MJX-monospace { font-family: var(--monospace); }
.MathJax_SVG .MJX-sans-serif { font-family: sans-serif; }
.MathJax_SVG { display: inline; font-style: normal; font-weight: 400; line-height: normal; zoom: 90%; text-indent: 0px; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; overflow-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; padding: 0px; margin: 0px; }
.MathJax_SVG * { transition: none 0s ease 0s; }
.MathJax_SVG_Display svg { vertical-align: middle !important; margin-bottom: 0px !important; margin-top: 0px !important; }
.os-windows.monocolor-emoji .md-emoji { font-family: "Segoe UI Symbol", sans-serif; }
.md-diagram-panel > svg { max-width: 100%; }
[lang="mermaid"] svg, [lang="flow"] svg { max-width: 100%; height: auto; }
[lang="mermaid"] .node text { font-size: 1rem; }
table tr th { border-bottom: 0px; }
video { max-width: 100%; display: block; margin: 0px auto; }
iframe { max-width: 100%; width: 100%; border: none; }
.highlight td, .highlight tr { border: 0px; }
svg[id^="mermaidChart"] { line-height: 1em; }


:root { --side-bar-bg-color: #fafafa; --control-text-color: #777; }
html { font-size: 16px; }
body { font-family: "Open Sans", "Clear Sans", "Helvetica Neue", Helvetica, Arial, sans-serif; color: rgb(51, 51, 51); line-height: 1.6; }
#write { max-width: 860px; margin: 0px auto; padding: 30px 30px 100px; }
#write > ul:first-child, #write > ol:first-child { margin-top: 30px; }
a { color: rgb(65, 131, 196); }
h1, h2, h3, h4, h5, h6 { position: relative; margin-top: 1rem; margin-bottom: 1rem; font-weight: bold; line-height: 1.4; cursor: text; }
h1:hover a.anchor, h2:hover a.anchor, h3:hover a.anchor, h4:hover a.anchor, h5:hover a.anchor, h6:hover a.anchor { text-decoration: none; }
h1 tt, h1 code { font-size: inherit; }
h2 tt, h2 code { font-size: inherit; }
h3 tt, h3 code { font-size: inherit; }
h4 tt, h4 code { font-size: inherit; }
h5 tt, h5 code { font-size: inherit; }
h6 tt, h6 code { font-size: inherit; }
h1 { padding-bottom: 0.3em; font-size: 2.25em; line-height: 1.2; border-bottom: 1px solid rgb(238, 238, 238); }
h2 { padding-bottom: 0.3em; font-size: 1.75em; line-height: 1.225; border-bottom: 1px solid rgb(238, 238, 238); }
h3 { font-size: 1.5em; line-height: 1.43; }
h4 { font-size: 1.25em; }
h5 { font-size: 1em; }
h6 { font-size: 1em; color: rgb(119, 119, 119); }
p, blockquote, ul, ol, dl, table { margin: 0.8em 0px; }
li > ol, li > ul { margin: 0px; }
hr { height: 2px; padding: 0px; margin: 16px 0px; background-color: rgb(231, 231, 231); border: 0px none; overflow: hidden; box-sizing: content-box; }
li p.first { display: inline-block; }
ul, ol { padding-left: 30px; }
ul:first-child, ol:first-child { margin-top: 0px; }
ul:last-child, ol:last-child { margin-bottom: 0px; }
blockquote { border-left: 4px solid rgb(223, 226, 229); padding: 0px 15px; color: rgb(119, 119, 119); }
blockquote blockquote { padding-right: 0px; }
table { padding: 0px; word-break: initial; }
table tr { border-top: 1px solid rgb(223, 226, 229); margin: 0px; padding: 0px; }
table tr:nth-child(2n), thead { background-color: rgb(248, 248, 248); }
table tr th { font-weight: bold; border-width: 1px 1px 0px; border-top-style: solid; border-right-style: solid; border-left-style: solid; border-top-color: rgb(223, 226, 229); border-right-color: rgb(223, 226, 229); border-left-color: rgb(223, 226, 229); border-image: initial; border-bottom-style: initial; border-bottom-color: initial; margin: 0px; padding: 6px 13px; }
table tr td { border: 1px solid rgb(223, 226, 229); margin: 0px; padding: 6px 13px; }
table tr th:first-child, table tr td:first-child { margin-top: 0px; }
table tr th:last-child, table tr td:last-child { margin-bottom: 0px; }
.CodeMirror-lines { padding-left: 4px; }
.code-tooltip { box-shadow: rgba(0, 28, 36, 0.3) 0px 1px 1px 0px; border-top: 1px solid rgb(238, 242, 242); }
.md-fences, code, tt { border: 1px solid rgb(231, 234, 237); background-color: rgb(248, 248, 248); border-radius: 3px; padding: 2px 4px 0px; font-size: 0.9em; }
code { background-color: rgb(243, 244, 244); padding: 0px 2px; }
.md-fences { margin-bottom: 15px; margin-top: 15px; padding-top: 8px; padding-bottom: 6px; }
.md-task-list-item > input { margin-left: -1.3em; }
@media print {
  html { font-size: 13px; }
  table, pre { break-inside: avoid; }
  pre { overflow-wrap: break-word; }
}
.md-fences { background-color: rgb(248, 248, 248); }
#write pre.md-meta-block { padding: 1rem; font-size: 85%; line-height: 1.45; background-color: rgb(247, 247, 247); border: 0px; border-radius: 3px; color: rgb(119, 119, 119); margin-top: 0px !important; }
.mathjax-block > .code-tooltip { bottom: 0.375rem; }
.md-mathjax-midline { background: rgb(250, 250, 250); }
#write > h3.md-focus::before { left: -1.5625rem; top: 0.375rem; }
#write > h4.md-focus::before { left: -1.5625rem; top: 0.285714rem; }
#write > h5.md-focus::before { left: -1.5625rem; top: 0.285714rem; }
#write > h6.md-focus::before { left: -1.5625rem; top: 0.285714rem; }
.md-image > .md-meta { border-radius: 3px; padding: 2px 0px 0px 4px; font-size: 0.9em; color: inherit; }
.md-tag { color: rgb(167, 167, 167); opacity: 1; }
.md-toc { margin-top: 20px; padding-bottom: 20px; }
.sidebar-tabs { border-bottom: none; }
#typora-quick-open { border: 1px solid rgb(221, 221, 221); background-color: rgb(248, 248, 248); }
#typora-quick-open-item { background-color: rgb(250, 250, 250); border-color: rgb(254, 254, 254) rgb(229, 229, 229) rgb(229, 229, 229) rgb(238, 238, 238); border-style: solid; border-width: 1px; }
.on-focus-mode blockquote { border-left-color: rgba(85, 85, 85, 0.12); }
header, .context-menu, .megamenu-content, footer { font-family: "Segoe UI", Arial, sans-serif; }
.file-node-content:hover .file-node-icon, .file-node-content:hover .file-node-open-state { visibility: visible; }
.mac-seamless-mode #typora-sidebar { background-color: var(--side-bar-bg-color); }
.md-lang { color: rgb(180, 101, 77); }
.html-for-mac .context-menu { --item-hover-bg-color: #E6F0FE; }
#md-notification .btn { border: 0px; }
.dropdown-menu .divider { border-color: rgb(229, 229, 229); }
.ty-preferences .window-content { background-color: rgb(250, 250, 250); }
.ty-preferences .nav-group-item.active { color: white; background: rgb(153, 153, 153); }


</style>
</head>
<body class='typora-export os-windows' >
<div  id='write'  class = 'is-node'><p>&nbsp;</p><p>&nbsp;</p><p align="center">
    <font size="6"><b>CAPS Notes - Lecture 2</b></font><br>
    <font size="6"><b>Supervised Learning</b></font><br>
    <br>
    Leo Klenner, Henry Fung, Cory Combs<br>
    <br>
    Last updated: 11/2/2019
</p><br><br><br><h2><a name="overview" class="md-header-anchor"></a><span>Overview</span></h2><p><span>In lecture 2, we begin our foray into supervised learning. To begin, we start with a crucial component of the infrastructure that allows us to use machine learning: data. From there, we explore model construction, applications, and the means to assess machine learning systems .</span></p><p><span>As we enter the technical portion of the course, we aim to provide the means to engage with engineers and to understand the power and limitations of these systems. There are books worth of material for each of these topics, and we emphasize that the coverage here is far from complete; rather, it reflects our assessment of what you </span><em><span>need</span></em><span> to know to get started in AI policy and strategy, and aims to provide solid launching points for continued engagement.</span></p><p><span>The session 2 notes cover:</span></p><ul><li><span>Data</span></li><li><span>The branches of supervised learning</span></li><li><span>The basics of supervised learning model construction</span></li><li><span>Regression models</span></li><li><span>Classification models</span></li><li><span>Tree-based models for classification and regression</span></li><li><span>Model evaluation</span></li><li><span>Data labeling system case study</span></li></ul><blockquote><p><em><span>Information is the oil of the 21st century, and analytics is the combustion engine.</span></em></p><p><em><span>- Peter Sondergaard</span></em><span> </span></p></blockquote><br><h2><a name="1-engaging-data" class="md-header-anchor"></a><span>1. Engaging Data</span></h2><h3><a name="modes-of-engagement" class="md-header-anchor"></a><span>Modes of Engagement</span></h3><p><span>In a course, say that an instructor gives you sample problems. You try to solve them, then the instructor informs you whether your answers are correct or incorrect. If anything is amiss, you try again, adjusting your responses. You are not taught new methods; you are only told that your answers were incorrect and you must try again using the given methods. In an ideal world, you continue to practice and iterate until your success rate is quite high and you can successfully answer new problems of the same form. This is a human analogy to </span><strong><span>supervised learning</span></strong><span>.</span></p><p><span>In contrast, imagine an pattern-finding exercise in which you have no training problems, no context, and no feedback. You are alone with your observations and your logical capabilities. What patterns will you identify? No doubt reasonable results may differ between people with slightly different processes - and all could, in principle, lead to different insights. This is a human analogy to </span><strong><span>unsupervised learning</span></strong><span>.</span></p><p><span>Moving from analogies to more precise machine learning specifications:</span></p><ul><li><strong><span>Supervised learning</span></strong><span> allows us to train machines that </span><em><span>learn from labeled data</span></em><span>. This includes making inferences and predictions about new, unlabeled observations based on previous observations. This includes regression and classification.</span></li><li><strong><span>Unsupervised learning</span></strong><span> allows untrained machines to autonomously </span><em><span>learn about unlabeled data</span></em><span>. This includes pattern recognition and categorization. (The distinction between categorization and classification will become clear in the next session.)</span></li></ul><p><strong><span>Today, we focus on supervised learning</span></strong><span>, the most commonly deployed form of machine learning in current industry and a basis of much economic and financial analysis. Supervised learning is highly restricted by the available data, requiring not only accurately labeled data appropriate for each unique use case, but very large amounts of it. These requirements pose significant challenges to achieving &quot;good&quot; outcomes, for which we will focus on three criteria: </span><em><span>correct</span></em><span> inferences, </span><em><span>useful</span></em><span> results, and </span><em><span>ethical</span></em><span> implementation. All of these come down, at some point, to the data used to train the model.</span></p><h3><a name="features-and-observations" class="md-header-anchor"></a><span>Features and Observations</span></h3><p><span>We can decompose all datasets into two components:</span></p><ul><li><strong><span>Features</span></strong><span>: the variables in a dataset (the columns in a table).</span></li><li><strong><span>Observations</span></strong><span>: the number of unique records (the rows in a table).</span></li></ul><p><span>As an example:</span></p><figure><table><thead><tr><th><span>ID [Observation Number]</span></th><th><span>Province [Feature 1]</span></th><th><span>Security Perception [Feature 2]</span></th></tr></thead><tbody><tr><td><span>3032</span></td><td><span>Badghis</span></td><td><span>3</span></td></tr><tr><td><span>3033</span></td><td><span>Samangan</span></td><td><span>2</span></td></tr><tr><td><span>3034</span></td><td><span>Wardak</span></td><td><span>5</span></td></tr></tbody></table></figure><p align="center">
    <i>Table 1:	Example compilation from USAID MISTI Survey Dataset features. A security perception value of 1 indicates a response of "improved a lot"; 2 of "improved a little"; 3 of "stayed the same"; 4 of "worsened a little"; and 5 of "worsened a lot". Explore the MISTI dataset here: <a href="https://catalog.data.gov/dataset/measuring-impact-of-stabilization-initiatives-survey-data" target="_blank">1</a>.
</i></p><p><span>Note that the data may include both continuous (numerical) and categorical variables (e.g. province names). Machine learning models require numerical data for calculation purposes; to achieve this, categorical variables are transformed into numerical values for training and evaluation purposes. As Table 1 demonstrates, some datasets may already represent some categorical data in numerical form, while leaving other entries in their original form. Adapting variables for modeling is part of the data cleaning and manipulation process and is taught as part of data science.</span></p><h4><a name="labels" class="md-header-anchor"></a><span>Labels</span></h4><p><span>Where do </span><em><span>labels</span></em><span> come into play?</span></p><ul><li><strong><span>Labels refer to target variables.</span></strong><span> To say that a supervised learner &quot;trains on labeled data&quot; means that it learns to correlate a specified target variable with the selected features. When a supervised learner predicts a target variable, it is predicting a label.</span></li><li><span>If no variable serves as a valid target variable, the problem is not suitable for supervised learning (unless or until a suitable target variable is added).</span></li><li><span>Features selected to predict the target variable become the hypothesized independent variables.</span></li></ul><p><span>To restate the supervised vs. unsupervised distinction:</span></p><ul><li><span>Supervised learners train to </span><em><span>predict</span></em><span> or </span><em><span>infer</span></em><span> a target variable based on a range of features, or independent variables, using examples for which the target variables are already known.</span></li><li><span>Unsupervised learners do not train, and run various algorithms to find patterns among the features.</span></li></ul><p><span>Given real-world datasets, this poses an important question: if you stumble upon data &quot;in the wild&quot;, how do you know whether it is suitable for supervised learning? In principle, the solution has two components: the question you want to answer, and a judgment call about the features available. If you want a model to learn to </span><em><span>predict</span></em><span> a target variable, you will pursue supervised learning; however, you must then separately assess whether the target variable </span><em><span>can be reasonably well predicted</span></em><span> given the available features. This requires hypothesis testing of the form familiar from regression analysis, much of which is best served by domain expertise.</span></p><p><span>Many datasets are constructed for sake of answering a particular question. Most well-designed experimental studies produce results with explicit target variables and features selected through principled hypothesis formation. Such experimental data collection is expensive and usually not conducted without rigorous specifications. On the other hand, less targeted large-scale data collection is more and more accessible, particularly in the world of &quot;big data&quot;; here, data may be collected without clear prior expectations. Data science is in large part the science of parsing datasets for useful results. Another major component is cleaning and preparing the data for use. Selecting, understanding, and processing data are essential foundations for all machine learning.</span></p><blockquote><p><span>*In God we trust. All others must bring data.**</span></p><p><em><span>*-  W. Edwards Deming</span></em></p></blockquote><h4><a name="better-data" class="md-header-anchor"></a><span>&quot;Better&quot; data</span></h4><p><span>One often hears that more data beats a better model. In general, this is on the right track. But this generalization requires a vital caveat: more data is </span><em><span>not automatically</span></em><span> better than less data. Rather, more </span><em><span>relevant</span></em><span> data is better than less applicable data. More bad data changes nothing. While this sounds quite obvious, when researching solutions to a problem it can be tempting to go after more readily available datasets - or the data you seek may not be available at all, making a potential proxy seem all but necessary. Choosing relevant data is, ultimately, a matter of </span><strong><span>feature selection</span></strong><span>: what are you asking your model to consider, and why?</span></p><p><strong><span>Machine learning follows the precept: Garbage In, Garbage Out.</span></strong></p><p><span>Irrelevant features, or even partially relevant features, can cause a model to make inferences that do not hold beyond the training set. In a human analogy, it is taught an inaccurate view of how the world works. Given new data, its predictions and inferences will not hold.</span></p><p><span>Given relevant features, the model still needs sufficient examples of each feature to be able to discern signal from noise. Given the noisiness of the real world, &quot;sufficient&quot; can often means tens of thousands up to millions of data points, if not more.</span></p><p><span>A key take-away for assessing supervised learning models:</span></p><ul><li><strong><span>Appropriate feature selection</span></strong><span> and </span><strong><span>numerous observations for each feature</span></strong><span> are </span><strong><span>joint necessities</span></strong><span> for successful models.</span></li></ul><p><span>In the final section, we will explore formal metrics of model performance. However, while we have robust tools to assess whether a model performs well, identifying features to measure and test in the first place can be as much art as science. Domain expertise serves a vital role in feature selection.</span></p><h3><a name="key-data-risks-overfitting-and-bias" class="md-header-anchor"></a><span>Key Data Risks: Overfitting and Bias</span></h3><p><span>We now turn to two of the most critical data-related risks in supervised learning: overfitting and bias. The former is primarily a function of how the algorithm learns from training data, and can be adjusted without changing the data. The latter is primarily a function of data selection, and generally requires changing what data is used to train the model.</span></p><p><strong><span>Overfitting</span></strong><span> indicates that a model has learned to map </span><em><span>noise</span></em><span>, rather than real, meaningful </span><em><span>signals</span></em><span> in the training data. An overfitted model will reflect trends in the training data pool that </span><em><span>do not exist</span></em><span> in other data pools. In other words, an overfitted model will make inaccurate predictions for the data you want to analyze, for example showing 99% accuracy on training data but merely 50% on new data. (Underfitting is also possible, though a much less common issue in today&#39;s data-rich era.) Overfitting is something we can - and must - test for and guard against, using a variety of metrics depending on the context. We will discuss key metrics shortly.</span></p><p><strong><span>Bias</span></strong><span> in the algorithmic context is, most simply, a systematic skewing of results. The term is fraught with other meanings, however, from the purely mathematical to the purely ethical, with different roles even across machine learning types (e.g. it is an essential ingredient of neural networks). As such, when people speak of &quot;bias&quot; in AI, they can mean many things. (This is enabled by a history of jargon adoption across fields - hence the importance of being specific in these discussions!) For now, we will consider the general, technical sense of bias as a skewing of results to a examine a key property of predictive supervised learning models: the bias-variance trade-off. Bias in the sense of AI ethics is discussed in a future session.</span></p><br><h2><a name="2-data-collection-under-uncertainty" class="md-header-anchor"></a><span>2. Data Collection Under Uncertainty</span></h2><p><span>Data is the heart of all analysis; turning data into useful insight is our central goal. With the above content all in place, we turn to a case study on the challenge of collecting relevant data for open-ended problems. </span></p><br><h2><a name="3-two-branches-classification-and-regression" class="md-header-anchor"></a><span>3. Two Branches: Classification and Regression</span></h2><p><span>Supervised learning has two principal branches:</span></p><ul><li><strong><span>Classification</span></strong><span>, in which the model predicts a categorical target variable.</span></li><li><strong><span>Regression</span></strong><span>, in which the model predicts a continuous (numerical) target variable.</span></li></ul><p><span>Consider two representative situations:</span></p><p><strong><span>1a) </span></strong><span> </span><em><span>You are a loan officer responsible for determining whether to approve appplications based on whether each applicant is likely to default. The market you are responsible for has been historically stable and you have tens of thousands of records on hand to judge past experience of similar applicants.</span></em></p><p><strong><span>2a)</span></strong><span> </span><em><span>You are a USAID analyst evaluating the impact of renewable energy credits on household emissions outcomes. You have data from a well-designed randomized control trial collected from tens of thousands of households representing the majority of the population.</span></em></p><p><span>What modes of analysis are most appropriate for these situations?</span></p><ul><li><span>Situation 1a) calls for </span><strong><span>classification</span></strong><span>: to predict whether the given applicant is likely to default on their loan.</span></li><li><span>Situation 2a) calls for </span><strong><span>regression</span></strong><span>: to test statistical correlations between factors. The target is a continuous variable.</span></li></ul><p><strong><span>In every supervised learning case, we should begin with two key questions:</span></strong></p><ul><li><span>Are we likely to have </span><strong><span>enough observations</span></strong><span> for robust results?</span></li><li><span>Are we likely to have </span><strong><span>relevant features</span></strong><span> for robust results?</span></li></ul><p><span>Each is a complex question and the subject of considerable study. In practice, this should be an opening discussion with the engineers on your team. For now, we will assume that tens of thousands of observations is sufficiently robust for both problems on the basis that we have a statistically representative spread of results in each case.</span></p><p><span>For the relevant features, the question is open: in an ideal world, </span><strong><span>what features would </span><em><span>you</span></em><span> select</span></strong><span>? What are </span><strong><span>potential issues with each</span></strong><span>?</span></p><p><span>Now consider variants of the above two situations, the first for classification and the second for regression:</span></p><p><strong><span>1b)</span></strong><span> </span><em><span>You are a loan specialist developing a national micro-lending platform for an emerging market with historically limited small-scale lending. You have a small pool of data from low-n trial programs in three cities.</span></em></p><p><strong><span>2b)</span></strong><span> </span><em><span>You are a State Department analyst assessing the impact of a stabilization initiative on local security using survey data. You have just under one thousand responses collected by volunteers in their home regions.</span></em></p><p><span>Revisit the two key questions regarding observations and features. We now have new, serious concerns about the data.</span></p><p><span>For situation1b):</span></p><ul><li><span>Is there enough historical data for statistically significant results?</span></li><li><span>Is the market changing in such a way that the existing data will no longer be representative, i.e. the values for the same features would be different today?</span></li><li><span>Is the market is changing such that different features are relevant?</span></li></ul><p><span>For situation 2b):</span></p><ul><li><span>Are the survey results sufficiently complete for statistically significant results (considering NA values and missing entries)?</span></li><li><span>Given the subjective nature of survey data, what can we, and can we not, infer from the dataset?</span></li><li><span>How can and should we assess the quality and representativeness of the data?</span></li></ul><p><span>Some questions, e.g. for the survey data, should be accounted for by research design. Non-experimental data, however, rely almost solely on separate research and domain expertise.</span></p><p><span>Based on the new contexts, what features would you select for each case? How might the different </span><em><span>environments</span></em><span> change your approach to feature selection? There are a range of reasonable responses. The goal for this discussion is simply to open the proverbial box.</span></p><br><h2><a name="4-introduction-to-supervised-learning-model-creation" class="md-header-anchor"></a><span>4. Introduction to Supervised Learning Model Creation</span></h2><h3><a name="basics-of-model-development" class="md-header-anchor"></a><span>Basics of Model Development</span></h3><p><span>We turn now to the foundations of supervised learning model development. This knowledge empowers direct engagement with engineers during the development process and anchors our understanding of how keys policy-relevant risks arise from model implementation.</span></p><p><span>A highly simplified template of model development is as follows:</span></p><ol start='' ><li><span>Data preparation</span></li><li><span>Model construction</span></li><li><span>Model training</span></li><li><span>Model testing</span></li><li><span>Model deployment</span></li></ol><p><span>Data preparation - including cleaning and manipulation - is the cornerstone of all data science. Without it, no model development is feasible. Model training and testing are highly iterative; based on evaluation, a model will undergo multiple reconfigurations to maximize accuracy, mitigate bias, and otherwise optimize performance. Deployment indicates that a model has gone live and is being used as intended; at this stage, as the Knight Capital example demonstrated, changes must be made with great caution.</span></p><p><span>In the next sections, we explore model construction, training, and testing, as the most essential components for later policy considerations.</span></p><h4><a name="high-level-model-construction-overview" class="md-header-anchor"></a><span>High-level model construction overview</span></h4><p><span>Machine learning models involve sophisticated algorithms that implement a variety of statistical and mathematical tools. Happily, the machine learning field has developed and made public a variety of tools that wrap these algorithms into reusable, high-level processes. Following the data cleaning and manipulation stage, baseline model construction follows a tidy prescription:</span></p><ol start='' ><li><strong><span>Split</span></strong><span> the data into training and test datasets</span></li><li><strong><span>Build</span></strong><span> the model</span></li><li><strong><span>Fit</span></strong><span> the model to the training data</span></li><li><strong><span>Predict</span></strong><span> values using the test data</span></li><li><strong><span>Evaluate</span></strong><span> the model&#39;s performance</span></li></ol><p><span>Let&#39;s explore the logic behind the procedure.</span></p><h4><a name="train-test-split" class="md-header-anchor"></a><span>Train-test-split</span></h4><p><span>Supervised learning trains on labeled data. However, in addition to training the model on labeled data, we need to </span><em><span>test</span></em><span> it on labeled data to properly assess its performance. Hence, rather than training on all available data, we want to set aside a subset of the data that is hidden from the model. Hence, we split our data into two subsets, traditionally called </span><em><span>train</span></em><span> and </span><em><span>test</span></em><span>. (For those in the know: we are setting aside validation datasets for the time being.)</span></p><p><span>As the names imply, the model will be trained on </span><em><span>train</span></em><span> and then tested on </span><em><span>test</span></em><span>; if the performance using the training set is much higher than on the test set, the model is likely overfitting to the training data and failing to generalize to the test set.</span></p><p><span>The proportion of the dataset assigned to each of the train and test datasets can vary. Assuming the features are relevant and well designed, more training data </span><em><span>generally</span></em><span> means better performance in the long run. However, more test data enables better insight into potential overfitting and underfitting, enabling designers to correct errors before the model is deployed. This direct trade-off is constant and unavoidable, and mitigated most simply by ensuring a large enough labeled dataset.</span></p><p><span>What about inconsistencies from different splits? If you are thinking that the </span><em><span>way</span></em><span> the data is split might affect the outcomes, you are absolutely correct. How can this be overcome? One simple technique is known as </span><strong><span>cross-validation</span></strong><span>. Cross-validation splits the data into a specified number of </span><em><span>different</span></em><span> training and test sets; trains and tests the model using each split; and compares the results of each case. The outcomes may be averaged or dealt with in any number of related ways, with the outcome of smoothing the potential for bias in any given random selection.</span></p><p><span>As a key take-away: if a potential problem is statistical in nature, ask the engineer on your team how it is addressed, or if it comes with a trade-off, how it is balanced. From the policy side, there may be outstanding problems posed by unique circumstances; statistical challenges, on the other hand, are frequently endemic to the modeling process, meaning that much research has gone into their management.</span></p><h4><a name="instantiate-fit-predict-evaluate" class="md-header-anchor"></a><span>Instantiate, fit, predict, evaluate</span></h4><p><span>With the reasoning behind the train-test-split step in place, the rest of the procedure is fairly predictable.</span></p><p><strong><span>Instantiating</span></strong><span> the model is a formal Python way to say &quot;tell your program which machine learning model to use&quot;. We will explore key options shortly. Most contemporary model construction is abstracted, meaning that, rather than writing out lines of linear algebra and calculus, we use existing code libraries and packages that have all of those details saved, letting us worry only about the data and specific parameters of interest. For example, we might specify &quot;logistic regression&quot; or &quot;random forest&quot; as our model.</span></p><p><strong><span>Fitting</span></strong><span> the model means training the model on the training data. This is also where the terms &quot;underfitting&quot; and &quot;overfitting&quot; come in. Supervised learning models work to minimize error rates (also known as loss); in other words, to produce results close to the true values, i.e. fitting results to the truth. &quot;Training&quot; is a more general term: you can overfit a model, but it isn&#39;t &quot;overtraining&quot; the model - it is simply training it in a way that leads to suboptimal performance.</span></p><p><strong><span>Predicting</span></strong><span> refers to running the trained model on the test data. In this phase, the model predicts the test data&#39;s labels.</span></p><p><strong><span>Evaluation</span></strong><span> compares the model&#39;s predicted labels to the actual test data labels to see how well the model performed. Evaluation should occur after every round of training. The results of evaluation inform the designers how they should change the model or various parameters to improve performance. There are myriad metrics used to evaluate models, depending on the model type and specific application. There is no magic number for any metric; instead designers will look to maximize particular metrics of interest, or achieve a balance among measurable trade-offs, depending on the goals. Evaluation is one of the most essential topics in modeling.</span></p><br><h2><a name="5-regression-models" class="md-header-anchor"></a><span>5. Regression Models</span></h2><p><span>Regression is the bread and butter of continuous variable prediction. At its core, regression models involve finding a line of best fit among a group of data based on established correlations between variables, enabling new data to be placed along the line based on one dimension (a selected feature) to estimate the likely value in the other dimension (the target variable).</span></p><p><span>Linear regression is the most common form of regression. More complex models are abundant, and often lead to higher accuracy among the training data, but are highly prone to overfitting and failing to generalize to new data. As such, these are generally only used in cases of a well-established nonlinear correlation.</span></p><p><span>Mathematically, linear regressions relate independent variables (features) </span><span class="MathJax_SVG" tabindex="-1" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="2.546ex" height="1.76ex" viewBox="0 -504.6 1096.3 757.9" role="img" focusable="false" style="vertical-align: -0.588ex;"><defs><path stroke-width="0" id="E11-MJMATHI-78" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path><path stroke-width="0" id="E11-MJMATHI-6E" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="#E11-MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="#E11-MJMATHI-6E" x="808" y="-213"></use></g></svg></span><script type="math/tex">x_n</script><span> to the dependent variable (target variable) </span><span class="MathJax_SVG" tabindex="-1" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.154ex" height="1.877ex" viewBox="0 -504.6 497 808.1" role="img" focusable="false" style="vertical-align: -0.705ex;"><defs><path stroke-width="0" id="E34-MJMATHI-79" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="#E34-MJMATHI-79" x="0" y="0"></use></g></svg></span><script type="math/tex">y</script><span> in the following general function:</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n215" cid="n215" mdtype="math_block"><div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-2-Frame" tabindex="-1" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="37.371ex" height="2.577ex" viewBox="0 -806.1 16090.2 1109.7" role="img" focusable="false" style="vertical-align: -0.705ex; max-width: 100%;"><defs><path stroke-width="0" id="E2-MJMATHI-79" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path stroke-width="0" id="E2-MJMAIN-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path stroke-width="0" id="E2-MJMATHI-3B2" d="M29 -194Q23 -188 23 -186Q23 -183 102 134T186 465Q208 533 243 584T309 658Q365 705 429 705H431Q493 705 533 667T573 570Q573 465 469 396L482 383Q533 332 533 252Q533 139 448 65T257 -10Q227 -10 203 -2T165 17T143 40T131 59T126 65L62 -188Q60 -194 42 -194H29ZM353 431Q392 431 427 419L432 422Q436 426 439 429T449 439T461 453T472 471T484 495T493 524T501 560Q503 569 503 593Q503 611 502 616Q487 667 426 667Q384 667 347 643T286 582T247 514T224 455Q219 439 186 308T152 168Q151 163 151 147Q151 99 173 68Q204 26 260 26Q302 26 349 51T425 137Q441 171 449 214T457 279Q457 337 422 372Q380 358 347 358H337Q258 358 258 389Q258 396 261 403Q275 431 353 431Z"></path><path stroke-width="0" id="E2-MJMAIN-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path><path stroke-width="0" id="E2-MJMAIN-2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path><path stroke-width="0" id="E2-MJMAIN-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path stroke-width="0" id="E2-MJMATHI-78" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path><path stroke-width="0" id="E2-MJMAIN-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path stroke-width="0" id="E2-MJMAIN-2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path><path stroke-width="0" id="E2-MJMATHI-6E" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path><path stroke-width="0" id="E2-MJMATHI-3F5" d="M227 -11Q149 -11 95 41T40 174Q40 262 87 322Q121 367 173 396T287 430Q289 431 329 431H367Q382 426 382 411Q382 385 341 385H325H312Q191 385 154 277L150 265H327Q340 256 340 246Q340 228 320 219H138V217Q128 187 128 143Q128 77 160 52T231 26Q258 26 284 36T326 57T343 68Q350 68 354 58T358 39Q358 36 357 35Q354 31 337 21T289 0T227 -11Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="#E2-MJMATHI-79" x="0" y="0"></use><use xlink:href="#E2-MJMAIN-3D" x="774" y="0"></use><g transform="translate(1830,0)"><use xlink:href="#E2-MJMATHI-3B2" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="#E2-MJMAIN-30" x="800" y="-213"></use></g><use xlink:href="#E2-MJMAIN-2B" x="3072" y="0"></use><g transform="translate(4072,0)"><use xlink:href="#E2-MJMATHI-3B2" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="#E2-MJMAIN-31" x="800" y="-213"></use></g><g transform="translate(5092,0)"><use xlink:href="#E2-MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="#E2-MJMAIN-31" x="808" y="-213"></use></g><use xlink:href="#E2-MJMAIN-2B" x="6339" y="0"></use><g transform="translate(7340,0)"><use xlink:href="#E2-MJMATHI-3B2" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="#E2-MJMAIN-32" x="800" y="-213"></use></g><g transform="translate(8359,0)"><use xlink:href="#E2-MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="#E2-MJMAIN-32" x="808" y="-213"></use></g><use xlink:href="#E2-MJMAIN-2B" x="9385" y="0"></use><use xlink:href="#E2-MJMAIN-2E" x="10163" y="0"></use><use xlink:href="#E2-MJMAIN-2E" x="10607" y="0"></use><use xlink:href="#E2-MJMAIN-2E" x="11052" y="0"></use><use xlink:href="#E2-MJMAIN-2B" x="11497" y="0"></use><g transform="translate(12275,0)"><use xlink:href="#E2-MJMATHI-3B2" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="#E2-MJMATHI-6E" x="800" y="-213"></use></g><g transform="translate(13365,0)"><use xlink:href="#E2-MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="#E2-MJMATHI-6E" x="808" y="-213"></use></g><use xlink:href="#E2-MJMAIN-2B" x="14683" y="0"></use><use xlink:href="#E2-MJMATHI-3F5" x="15684" y="0"></use></g></svg></span></div><script type="math/tex; mode=display" id="MathJax-Element-2">y = \beta_0 + \beta_1x_1 + \beta_2x_2 + ... + \beta_nx_n + \epsilon</script></div></div><p><span>where </span><span class="MathJax_SVG" tabindex="-1" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="2.113ex" height="2.461ex" viewBox="0 -806.1 910 1059.4" role="img" focusable="false" style="vertical-align: -0.588ex;"><defs><path stroke-width="0" id="E21-MJMATHI-3B2" d="M29 -194Q23 -188 23 -186Q23 -183 102 134T186 465Q208 533 243 584T309 658Q365 705 429 705H431Q493 705 533 667T573 570Q573 465 469 396L482 383Q533 332 533 252Q533 139 448 65T257 -10Q227 -10 203 -2T165 17T143 40T131 59T126 65L62 -188Q60 -194 42 -194H29ZM353 431Q392 431 427 419L432 422Q436 426 439 429T449 439T461 453T472 471T484 495T493 524T501 560Q503 569 503 593Q503 611 502 616Q487 667 426 667Q384 667 347 643T286 582T247 514T224 455Q219 439 186 308T152 168Q151 163 151 147Q151 99 173 68Q204 26 260 26Q302 26 349 51T425 137Q441 171 449 214T457 279Q457 337 422 372Q380 358 347 358H337Q258 358 258 389Q258 396 261 403Q275 431 353 431Z"></path><path stroke-width="0" id="E21-MJMATHI-69" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="#E21-MJMATHI-3B2" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="#E21-MJMATHI-69" x="800" y="-213"></use></g></svg></span><script type="math/tex">\beta_i</script><span> values indicate the weights associated with each independent variable and </span><span class="MathJax_SVG" tabindex="-1" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="0.943ex" height="1.41ex" viewBox="0 -504.6 406 607.1" role="img" focusable="false" style="vertical-align: -0.238ex;"><defs><path stroke-width="0" id="E17-MJMATHI-3F5" d="M227 -11Q149 -11 95 41T40 174Q40 262 87 322Q121 367 173 396T287 430Q289 431 329 431H367Q382 426 382 411Q382 385 341 385H325H312Q191 385 154 277L150 265H327Q340 256 340 246Q340 228 320 219H138V217Q128 187 128 143Q128 77 160 52T231 26Q258 26 284 36T326 57T343 68Q350 68 354 58T358 39Q358 36 357 35Q354 31 337 21T289 0T227 -11Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="#E17-MJMATHI-3F5" x="0" y="0"></use></g></svg></span><script type="math/tex">\epsilon</script><span> (epsilon) indicates an error term. When given actual values for the </span><span class="MathJax_SVG" tabindex="-1" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="2.113ex" height="2.461ex" viewBox="0 -806.1 910 1059.4" role="img" focusable="false" style="vertical-align: -0.588ex;"><defs><path stroke-width="0" id="E21-MJMATHI-3B2" d="M29 -194Q23 -188 23 -186Q23 -183 102 134T186 465Q208 533 243 584T309 658Q365 705 429 705H431Q493 705 533 667T573 570Q573 465 469 396L482 383Q533 332 533 252Q533 139 448 65T257 -10Q227 -10 203 -2T165 17T143 40T131 59T126 65L62 -188Q60 -194 42 -194H29ZM353 431Q392 431 427 419L432 422Q436 426 439 429T449 439T461 453T472 471T484 495T493 524T501 560Q503 569 503 593Q503 611 502 616Q487 667 426 667Q384 667 347 643T286 582T247 514T224 455Q219 439 186 308T152 168Q151 163 151 147Q151 99 173 68Q204 26 260 26Q302 26 349 51T425 137Q441 171 449 214T457 279Q457 337 422 372Q380 358 347 358H337Q258 358 258 389Q258 396 261 403Q275 431 353 431Z"></path><path stroke-width="0" id="E21-MJMATHI-69" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="#E21-MJMATHI-3B2" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="#E21-MJMATHI-69" x="800" y="-213"></use></g></svg></span><script type="math/tex">\beta_i</script><span> terms, this becomes a </span><strong><span>hypothesis</span></strong><span> predicting how the variables are related.</span></p><p><span>The central task of linear regression is to calculate the coefficients, or weights, that optimally predict </span><span class="MathJax_SVG" tabindex="-1" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.154ex" height="1.877ex" viewBox="0 -504.6 497 808.1" role="img" focusable="false" style="vertical-align: -0.705ex;"><defs><path stroke-width="0" id="E34-MJMATHI-79" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="#E34-MJMATHI-79" x="0" y="0"></use></g></svg></span><script type="math/tex">y</script><span> based on the independent variables. That is, to find the best hypothesis, which will be applied to new data in the future.</span></p><p><span>&quot;Optimal&quot; prediction, as discussed in session 1, suggests &quot;minimal error&quot;. Here, the goal is to achieve a minimal error term </span><span class="MathJax_SVG" tabindex="-1" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="0.943ex" height="1.41ex" viewBox="0 -504.6 406 607.1" role="img" focusable="false" style="vertical-align: -0.238ex;"><defs><path stroke-width="0" id="E17-MJMATHI-3F5" d="M227 -11Q149 -11 95 41T40 174Q40 262 87 322Q121 367 173 396T287 430Q289 431 329 431H367Q382 426 382 411Q382 385 341 385H325H312Q191 385 154 277L150 265H327Q340 256 340 246Q340 228 320 219H138V217Q128 187 128 143Q128 77 160 52T231 26Q258 26 284 36T326 57T343 68Q350 68 354 58T358 39Q358 36 357 35Q354 31 337 21T289 0T227 -11Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="#E17-MJMATHI-3F5" x="0" y="0"></use></g></svg></span><script type="math/tex">\epsilon</script><span> across the space of predictions, which indicates maximal fit to the true values. At this point, the earlier-discussed considerations of overfitting apply, and engineers must work to balance model fit with overfitting, primarily by comparing performance on training data and test data. </span></p><p><span>Recall that the independent variables used for prediction are initially selected during feature selection. Statistical testing will ultimately help identify the best selection of features among the available options. However, it cannot suggest new features for collection or testing; for that, domain expertise is an essential input.</span></p><p><span>Other forms of regression, such as polynomial regression, are variations on this theme, typically incorporating nonlinear relationships such as powers of </span><span class="MathJax_SVG" tabindex="-1" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.329ex" height="1.41ex" viewBox="0 -504.6 572 607.1" role="img" focusable="false" style="vertical-align: -0.238ex;"><defs><path stroke-width="0" id="E18-MJMATHI-78" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="#E18-MJMATHI-78" x="0" y="0"></use></g></svg></span><script type="math/tex">x</script><span> and interaction terms such as </span><span class="MathJax_SVG" tabindex="-1" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="4.764ex" height="1.644ex" viewBox="0 -504.6 2051.1 707.6" role="img" focusable="false" style="vertical-align: -0.472ex;"><defs><path stroke-width="0" id="E19-MJMATHI-78" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path><path stroke-width="0" id="E19-MJMAIN-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path stroke-width="0" id="E19-MJMAIN-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="#E19-MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="#E19-MJMAIN-31" x="808" y="-213"></use><g transform="translate(1025,0)"><use xlink:href="#E19-MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="#E19-MJMAIN-32" x="808" y="-213"></use></g></g></svg></span><script type="math/tex">x_1x_2</script><span>; for example, polynomial regression is of the general form</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n221" cid="n221" mdtype="math_block"><div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-3-Frame" tabindex="-1" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="53.193ex" height="3.044ex" viewBox="0 -906.7 22902.5 1310.7" role="img" focusable="false" style="vertical-align: -0.938ex; max-width: 100%;"><defs><path stroke-width="0" id="E3-MJMATHI-79" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path stroke-width="0" id="E3-MJMAIN-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path stroke-width="0" id="E3-MJMATHI-3B2" d="M29 -194Q23 -188 23 -186Q23 -183 102 134T186 465Q208 533 243 584T309 658Q365 705 429 705H431Q493 705 533 667T573 570Q573 465 469 396L482 383Q533 332 533 252Q533 139 448 65T257 -10Q227 -10 203 -2T165 17T143 40T131 59T126 65L62 -188Q60 -194 42 -194H29ZM353 431Q392 431 427 419L432 422Q436 426 439 429T449 439T461 453T472 471T484 495T493 524T501 560Q503 569 503 593Q503 611 502 616Q487 667 426 667Q384 667 347 643T286 582T247 514T224 455Q219 439 186 308T152 168Q151 163 151 147Q151 99 173 68Q204 26 260 26Q302 26 349 51T425 137Q441 171 449 214T457 279Q457 337 422 372Q380 358 347 358H337Q258 358 258 389Q258 396 261 403Q275 431 353 431Z"></path><path stroke-width="0" id="E3-MJMAIN-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path><path stroke-width="0" id="E3-MJMAIN-2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path><path stroke-width="0" id="E3-MJMAIN-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path stroke-width="0" id="E3-MJMATHI-78" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path><path stroke-width="0" id="E3-MJMAIN-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path stroke-width="0" id="E3-MJMATHI-3F5" d="M227 -11Q149 -11 95 41T40 174Q40 262 87 322Q121 367 173 396T287 430Q289 431 329 431H367Q382 426 382 411Q382 385 341 385H325H312Q191 385 154 277L150 265H327Q340 256 340 246Q340 228 320 219H138V217Q128 187 128 143Q128 77 160 52T231 26Q258 26 284 36T326 57T343 68Q350 68 354 58T358 39Q358 36 357 35Q354 31 337 21T289 0T227 -11Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="#E3-MJMATHI-79" x="0" y="0"></use><use xlink:href="#E3-MJMAIN-3D" x="774" y="0"></use><g transform="translate(1830,0)"><use xlink:href="#E3-MJMATHI-3B2" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="#E3-MJMAIN-30" x="800" y="-213"></use></g><use xlink:href="#E3-MJMAIN-2B" x="3072" y="0"></use><g transform="translate(4072,0)"><use xlink:href="#E3-MJMATHI-3B2" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="#E3-MJMAIN-31" x="800" y="-213"></use></g><g transform="translate(5092,0)"><use xlink:href="#E3-MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="#E3-MJMAIN-31" x="808" y="-213"></use></g><use xlink:href="#E3-MJMAIN-2B" x="6339" y="0"></use><g transform="translate(7340,0)"><use xlink:href="#E3-MJMATHI-3B2" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="#E3-MJMAIN-32" x="800" y="-213"></use></g><g transform="translate(8359,0)"><use xlink:href="#E3-MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="#E3-MJMAIN-32" x="808" y="-213"></use></g><use xlink:href="#E3-MJMAIN-2B" x="9607" y="0"></use><g transform="translate(10607,0)"><use xlink:href="#E3-MJMATHI-3B2" x="0" y="0"></use><g transform="translate(566,-150)"><use transform="scale(0.707)" xlink:href="#E3-MJMAIN-31"></use><use transform="scale(0.707)" xlink:href="#E3-MJMAIN-31" x="500" y="0"></use></g></g><g transform="translate(11980,0)"><use xlink:href="#E3-MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="#E3-MJMAIN-32" x="808" y="487"></use><use transform="scale(0.707)" xlink:href="#E3-MJMAIN-31" x="808" y="-434"></use></g><use xlink:href="#E3-MJMAIN-2B" x="13228" y="0"></use><g transform="translate(14228,0)"><use xlink:href="#E3-MJMATHI-3B2" x="0" y="0"></use><g transform="translate(566,-150)"><use transform="scale(0.707)" xlink:href="#E3-MJMAIN-32"></use><use transform="scale(0.707)" xlink:href="#E3-MJMAIN-32" x="500" y="0"></use></g></g><g transform="translate(15601,0)"><use xlink:href="#E3-MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="#E3-MJMAIN-32" x="808" y="487"></use><use transform="scale(0.707)" xlink:href="#E3-MJMAIN-32" x="808" y="-434"></use></g><use xlink:href="#E3-MJMAIN-2B" x="16849" y="0"></use><g transform="translate(17849,0)"><use xlink:href="#E3-MJMATHI-3B2" x="0" y="0"></use><g transform="translate(566,-150)"><use transform="scale(0.707)" xlink:href="#E3-MJMAIN-31"></use><use transform="scale(0.707)" xlink:href="#E3-MJMAIN-32" x="500" y="0"></use></g></g><g transform="translate(19222,0)"><use xlink:href="#E3-MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="#E3-MJMAIN-31" x="808" y="-213"></use></g><g transform="translate(20248,0)"><use xlink:href="#E3-MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="#E3-MJMAIN-32" x="808" y="-213"></use></g><use xlink:href="#E3-MJMAIN-2B" x="21496" y="0"></use><use xlink:href="#E3-MJMATHI-3F5" x="22496" y="0"></use></g></svg></span></div><script type="math/tex; mode=display" id="MathJax-Element-3">y = \beta_0 + \beta_1x_1 + \beta_2x_2 + \beta_{11}x_1^2 + \beta_{22}x_2^2 + \beta_{12}x_1x_2 + \epsilon</script></div></div><p><span>where </span><span class="MathJax_SVG" tabindex="-1" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="2.79ex" height="2.694ex" viewBox="0 -806.1 1201.3 1160" role="img" focusable="false" style="vertical-align: -0.822ex;"><defs><path stroke-width="0" id="E20-MJMATHI-3B2" d="M29 -194Q23 -188 23 -186Q23 -183 102 134T186 465Q208 533 243 584T309 658Q365 705 429 705H431Q493 705 533 667T573 570Q573 465 469 396L482 383Q533 332 533 252Q533 139 448 65T257 -10Q227 -10 203 -2T165 17T143 40T131 59T126 65L62 -188Q60 -194 42 -194H29ZM353 431Q392 431 427 419L432 422Q436 426 439 429T449 439T461 453T472 471T484 495T493 524T501 560Q503 569 503 593Q503 611 502 616Q487 667 426 667Q384 667 347 643T286 582T247 514T224 455Q219 439 186 308T152 168Q151 163 151 147Q151 99 173 68Q204 26 260 26Q302 26 349 51T425 137Q441 171 449 214T457 279Q457 337 422 372Q380 358 347 358H337Q258 358 258 389Q258 396 261 403Q275 431 353 431Z"></path><path stroke-width="0" id="E20-MJMATHI-69" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path stroke-width="0" id="E20-MJMATHI-6A" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="#E20-MJMATHI-3B2" x="0" y="0"></use><g transform="translate(566,-150)"><use transform="scale(0.707)" xlink:href="#E20-MJMATHI-69" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="#E20-MJMATHI-6A" x="345" y="0"></use></g></g></svg></span><script type="math/tex">\beta_{ij}</script><span> values indicate interaction term weights of the same sort as </span><span class="MathJax_SVG" tabindex="-1" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="2.113ex" height="2.461ex" viewBox="0 -806.1 910 1059.4" role="img" focusable="false" style="vertical-align: -0.588ex;"><defs><path stroke-width="0" id="E21-MJMATHI-3B2" d="M29 -194Q23 -188 23 -186Q23 -183 102 134T186 465Q208 533 243 584T309 658Q365 705 429 705H431Q493 705 533 667T573 570Q573 465 469 396L482 383Q533 332 533 252Q533 139 448 65T257 -10Q227 -10 203 -2T165 17T143 40T131 59T126 65L62 -188Q60 -194 42 -194H29ZM353 431Q392 431 427 419L432 422Q436 426 439 429T449 439T461 453T472 471T484 495T493 524T501 560Q503 569 503 593Q503 611 502 616Q487 667 426 667Q384 667 347 643T286 582T247 514T224 455Q219 439 186 308T152 168Q151 163 151 147Q151 99 173 68Q204 26 260 26Q302 26 349 51T425 137Q441 171 449 214T457 279Q457 337 422 372Q380 358 347 358H337Q258 358 258 389Q258 396 261 403Q275 431 353 431Z"></path><path stroke-width="0" id="E21-MJMATHI-69" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="#E21-MJMATHI-3B2" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="#E21-MJMATHI-69" x="800" y="-213"></use></g></svg></span><script type="math/tex">\beta_i</script><span> values for non-interaction term weights.</span></p><p><span>Regression is a widespread tool, particularly in economics, and as such its role as a machine learning tool can be overlooked. However, it remains one of the most commonly deployed tools in data analysis and is a fundamental tool in supervised learning - including as the foundation of categorical variable classification, explored in the next section.</span></p><br><h2><a name="6-classification" class="md-header-anchor"></a><span>6. Classification</span></h2><h3><a name="foundations" class="md-header-anchor"></a><span>Foundations</span></h3><p><span>Classification models predict categorical target variables. For instance: does a given image show a cat or a dog? Should the loan application be approved or not? These two examples are cases of </span><strong><span>binary classification</span></strong><span>, where &quot;binary&quot; simply indicates two possible outputs, frequently represented numerically as 0 and 1. Logistic regression is the leading means of binary classification.</span></p><p><span>Take another question: what type of animal does a given image show? The question as posed is not well formed for a supervised learning classification model. Instead, the system requires specified options for the target variable. For instance, you might specify a list of animals an image might possibly show. This could then become a well-formed question of the form: which of the specified animals (i.e. categories X, Y, Z, ...) does this image show? This problem is a candidate for </span><strong><span>multiclass classification</span></strong><span>. This can be achieved through multinomial logistic regression and tree-based models such as random forest.</span></p><h3><a name="key-model-logistic-regression" class="md-header-anchor"></a><span>Key Model: Logistic Regression</span></h3><p><span>The archetypal model for </span><strong><span>binary classification</span></strong><span> is </span><strong><span>logistic regression</span></strong><span>. Note that, despite the name, logistic regression is a form of classification and not regression. To get started on this topic, let&#39;s look at a situation discussed earlier:</span></p><p><strong><span>1a) </span></strong><span> </span><em><span>You are a loan officer responsible for determining whether to approve appplications based on whether each applicant is likely to default.</span></em></p><p><span>The problem at hand calls for binary classification of applications: those to be approved and those to be rejected. We decide to base our decision on whether the applicant is deemed likely to default.</span></p><p><span>Looking at historical data, we see a feature called &quot;default&quot;, with values 1, indicating that the person defaulted, and 0, where they did not default. We set &quot;default&quot; as our target variable. How do we go about classifying each application as 1 or 0 based on selected features?</span></p><p><span>Logistic regression employs a logistic function to transform an array of possible values into a value close to 1 or 0. The function produces a probabilistic output, and the developer can decide a probability cutoff, known as the </span><strong><span>decision boundary</span></strong><span>, to classify values as 1 or 0.</span></p><p><span>The </span><strong><span>sigmoid function</span></strong><span> is the basic logistic function for binary classification:</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n237" cid="n237" mdtype="math_block"><div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-4-Frame" tabindex="-1" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="22.571ex" height="5.262ex" viewBox="-39 -1409.3 9718 2265.7" role="img" focusable="false" style="vertical-align: -1.989ex; margin-left: -0.091ex; max-width: 100%;"><defs><path stroke-width="0" id="E4-MJMATHI-70" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path><path stroke-width="0" id="E4-MJMAIN-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path stroke-width="0" id="E4-MJMAIN-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path stroke-width="0" id="E4-MJMAIN-2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path><path stroke-width="0" id="E4-MJMATHI-65" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path><path stroke-width="0" id="E4-MJMAIN-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path stroke-width="0" id="E4-MJMATHI-7A" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="#E4-MJMATHI-70" x="0" y="0"></use><use xlink:href="#E4-MJMAIN-3D" x="780" y="0"></use><g transform="translate(1558,0)"><g transform="translate(397,0)"><rect stroke="none" width="3289" height="60" x="0" y="220"></rect><use xlink:href="#E4-MJMAIN-31" x="1394" y="676"></use><g transform="translate(60,-686)"><use xlink:href="#E4-MJMAIN-31" x="0" y="0"></use><use xlink:href="#E4-MJMAIN-2B" x="722" y="0"></use><g transform="translate(1722,0)"><use xlink:href="#E4-MJMATHI-65" x="0" y="0"></use><g transform="translate(466,288)"><use transform="scale(0.707)" xlink:href="#E4-MJMAIN-2212" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="#E4-MJMATHI-7A" x="778" y="0"></use></g></g></g></g></g><use xlink:href="#E4-MJMAIN-3D" x="5643" y="0"></use><g transform="translate(6421,0)"><g transform="translate(397,0)"><rect stroke="none" width="2739" height="60" x="0" y="220"></rect><g transform="translate(921,676)"><use xlink:href="#E4-MJMATHI-65" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="#E4-MJMATHI-7A" x="659" y="513"></use></g><g transform="translate(60,-686)"><use xlink:href="#E4-MJMATHI-65" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="#E4-MJMATHI-7A" x="659" y="408"></use><use xlink:href="#E4-MJMAIN-2B" x="1119" y="0"></use><use xlink:href="#E4-MJMAIN-31" x="2119" y="0"></use></g></g></g></g></svg></span></div><script type="math/tex; mode=display" id="MathJax-Element-4">p = \frac{1}{1+e^{-z}} = \frac{e^z}{e^z+1}</script></div></div><p><span>where p is a predicted probability, which will end up close to 1 or 0 for the vast majority of inputs. (Both formulations are fairly common; they are listed together to avoid future confusion.) Before turning to explain the essential variable, </span><span class="MathJax_SVG" tabindex="-1" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.087ex" height="1.41ex" viewBox="0 -504.6 468 607.1" role="img" focusable="false" style="vertical-align: -0.238ex;"><defs><path stroke-width="0" id="E25-MJMATHI-7A" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="#E25-MJMATHI-7A" x="0" y="0"></use></g></svg></span><script type="math/tex">z</script><span>, let&#39;s see the general behavior of the sigmoid function:</span></p><p align="center">
    <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/480px-Logistic-curve.svg.png" max-height="350" alt="Sigmoid function"></p><p align="center">
    <i>Figure 1: The sigmoid function. Credit: Wikimedia Commons.
</i></p><p><span>Notice that the maximum y value of the sigmoid function is 1, and the minimum is 0. Outputs are essentially &quot;pushed&quot; toward one extreme or the other, allowing us to split the dataset.</span></p><p><span>What is </span><span class="MathJax_SVG" tabindex="-1" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.087ex" height="1.41ex" viewBox="0 -504.6 468 607.1" role="img" focusable="false" style="vertical-align: -0.238ex;"><defs><path stroke-width="0" id="E25-MJMATHI-7A" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="#E25-MJMATHI-7A" x="0" y="0"></use></g></svg></span><script type="math/tex">z</script><span>? As it turns out, </span><span class="MathJax_SVG" tabindex="-1" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.087ex" height="1.41ex" viewBox="0 -504.6 468 607.1" role="img" focusable="false" style="vertical-align: -0.238ex;"><defs><path stroke-width="0" id="E25-MJMATHI-7A" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="#E25-MJMATHI-7A" x="0" y="0"></use></g></svg></span><script type="math/tex">z</script><span> is simply a regression equation! That is, </span><span class="MathJax_SVG" tabindex="-1" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.087ex" height="1.41ex" viewBox="0 -504.6 468 607.1" role="img" focusable="false" style="vertical-align: -0.238ex;"><defs><path stroke-width="0" id="E25-MJMATHI-7A" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="#E25-MJMATHI-7A" x="0" y="0"></use></g></svg></span><script type="math/tex">z</script><span> is the </span><strong><span>hypothesis</span></strong><span> regarding the relationship between the target variable and independent variables. Again, the general form of the function, using a linear regression equation, will be:</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n242" cid="n242" mdtype="math_block"><div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-5-Frame" tabindex="-1" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="53.126ex" height="3.044ex" viewBox="0 -906.7 22873.5 1310.7" role="img" focusable="false" style="vertical-align: -0.938ex; max-width: 100%;"><defs><path stroke-width="0" id="E5-MJMATHI-7A" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path><path stroke-width="0" id="E5-MJMAIN-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path stroke-width="0" id="E5-MJMATHI-3B2" d="M29 -194Q23 -188 23 -186Q23 -183 102 134T186 465Q208 533 243 584T309 658Q365 705 429 705H431Q493 705 533 667T573 570Q573 465 469 396L482 383Q533 332 533 252Q533 139 448 65T257 -10Q227 -10 203 -2T165 17T143 40T131 59T126 65L62 -188Q60 -194 42 -194H29ZM353 431Q392 431 427 419L432 422Q436 426 439 429T449 439T461 453T472 471T484 495T493 524T501 560Q503 569 503 593Q503 611 502 616Q487 667 426 667Q384 667 347 643T286 582T247 514T224 455Q219 439 186 308T152 168Q151 163 151 147Q151 99 173 68Q204 26 260 26Q302 26 349 51T425 137Q441 171 449 214T457 279Q457 337 422 372Q380 358 347 358H337Q258 358 258 389Q258 396 261 403Q275 431 353 431Z"></path><path stroke-width="0" id="E5-MJMAIN-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path><path stroke-width="0" id="E5-MJMAIN-2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path><path stroke-width="0" id="E5-MJMAIN-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path stroke-width="0" id="E5-MJMATHI-78" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path><path stroke-width="0" id="E5-MJMAIN-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path stroke-width="0" id="E5-MJMATHI-3F5" d="M227 -11Q149 -11 95 41T40 174Q40 262 87 322Q121 367 173 396T287 430Q289 431 329 431H367Q382 426 382 411Q382 385 341 385H325H312Q191 385 154 277L150 265H327Q340 256 340 246Q340 228 320 219H138V217Q128 187 128 143Q128 77 160 52T231 26Q258 26 284 36T326 57T343 68Q350 68 354 58T358 39Q358 36 357 35Q354 31 337 21T289 0T227 -11Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="#E5-MJMATHI-7A" x="0" y="0"></use><use xlink:href="#E5-MJMAIN-3D" x="745" y="0"></use><g transform="translate(1801,0)"><use xlink:href="#E5-MJMATHI-3B2" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="#E5-MJMAIN-30" x="800" y="-213"></use></g><use xlink:href="#E5-MJMAIN-2B" x="3043" y="0"></use><g transform="translate(4043,0)"><use xlink:href="#E5-MJMATHI-3B2" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="#E5-MJMAIN-31" x="800" y="-213"></use></g><g transform="translate(5063,0)"><use xlink:href="#E5-MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="#E5-MJMAIN-31" x="808" y="-213"></use></g><use xlink:href="#E5-MJMAIN-2B" x="6310" y="0"></use><g transform="translate(7311,0)"><use xlink:href="#E5-MJMATHI-3B2" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="#E5-MJMAIN-32" x="800" y="-213"></use></g><g transform="translate(8330,0)"><use xlink:href="#E5-MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="#E5-MJMAIN-32" x="808" y="-213"></use></g><use xlink:href="#E5-MJMAIN-2B" x="9578" y="0"></use><g transform="translate(10578,0)"><use xlink:href="#E5-MJMATHI-3B2" x="0" y="0"></use><g transform="translate(566,-150)"><use transform="scale(0.707)" xlink:href="#E5-MJMAIN-31"></use><use transform="scale(0.707)" xlink:href="#E5-MJMAIN-31" x="500" y="0"></use></g></g><g transform="translate(11951,0)"><use xlink:href="#E5-MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="#E5-MJMAIN-32" x="808" y="487"></use><use transform="scale(0.707)" xlink:href="#E5-MJMAIN-31" x="808" y="-434"></use></g><use xlink:href="#E5-MJMAIN-2B" x="13199" y="0"></use><g transform="translate(14199,0)"><use xlink:href="#E5-MJMATHI-3B2" x="0" y="0"></use><g transform="translate(566,-150)"><use transform="scale(0.707)" xlink:href="#E5-MJMAIN-32"></use><use transform="scale(0.707)" xlink:href="#E5-MJMAIN-32" x="500" y="0"></use></g></g><g transform="translate(15572,0)"><use xlink:href="#E5-MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="#E5-MJMAIN-32" x="808" y="487"></use><use transform="scale(0.707)" xlink:href="#E5-MJMAIN-32" x="808" y="-434"></use></g><use xlink:href="#E5-MJMAIN-2B" x="16820" y="0"></use><g transform="translate(17820,0)"><use xlink:href="#E5-MJMATHI-3B2" x="0" y="0"></use><g transform="translate(566,-150)"><use transform="scale(0.707)" xlink:href="#E5-MJMAIN-31"></use><use transform="scale(0.707)" xlink:href="#E5-MJMAIN-32" x="500" y="0"></use></g></g><g transform="translate(19193,0)"><use xlink:href="#E5-MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="#E5-MJMAIN-31" x="808" y="-213"></use></g><g transform="translate(20219,0)"><use xlink:href="#E5-MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="#E5-MJMAIN-32" x="808" y="-213"></use></g><use xlink:href="#E5-MJMAIN-2B" x="21467" y="0"></use><use xlink:href="#E5-MJMATHI-3F5" x="22467" y="0"></use></g></svg></span></div><script type="math/tex; mode=display" id="MathJax-Element-5">z = \beta_0 + \beta_1x_1 + \beta_2x_2 + \beta_{11}x_1^2 + \beta_{22}x_2^2 + \beta_{12}x_1x_2 + \epsilon</script></div></div><p><span>Rather than predicting a value from a continuous range, we put this hypothesis into the sigmoid function (or any other logistic function) to predict either 1 or 0, corresponding to our labels of interest.</span></p><p><span>The same considerations of overfitting apply again. However, the particular metrics of interest can vary considerably depending on the dataset involved. We will explore this further in the metrics section.</span></p><p><span>Now we ask: how do you, as a loan officer, intend to approach the decisions? Following feature selection and model construction, two key questions remain.</span></p><h4><a name="data-imbalance" class="md-header-anchor"></a><span>Data imbalance</span></h4><p><span>First, data are frequently </span><strong><span>imbalanced</span></strong><span>: one group may have far more data than another. For example, default tends to be rare. If you had 995 examples of &quot;no default&quot; and a five examples of &quot;default&quot; in your training data, how accurate could your model be? How much could it learn? In practice, very little: you require far more instances of &quot;default&quot; to learn effectively.</span></p><p><span>However, even with robust samples (say, 100,000 of &quot;no default&quot; and 2,000 of &quot;default&quot;), the imbalance leads to a </span><strong><span>measurement problem</span></strong><span>. If you simply scrapped the system and declared </span><em><span>all</span></em><span> applications to show (incorrectly) &quot;no default&quot;, the </span><em><span>accuracy</span></em><span> of the results would be a healthy 98% - despite failing to acheive any intended goals. Something is clearly amiss. We will solve this problem shortly, when studying evaluation.</span></p><h4><a name="goal-specification" class="md-header-anchor"></a><span>Goal specification</span></h4><p><span>Second, a successful loan officer needs to know: what is the bank&#39;s or lender&#39;s </span><strong><span>risk tolerance</span></strong><span>? Mathematically, this question becomes: what is the appropriate </span><strong><span>decision boundary</span></strong><span>, i.e. the value above which we classify an application as 1 and below which we classify it as a 0? The value frequently is not a halfway point, but another value that is predicted to result in a desirable amount of lending (if that is a fixed end goal). This boundary-setting reflects the bank&#39;s goals and priorities. It may vary between banks, for a single bank at different points in time, for a single bank by each region of lending, etc. Modeling is rarely a one-and-done affair, but something that continues over time, both to improve on reaching a given goal and to refocus on new goals.</span></p><p><span>As a final note, </span><strong><span>multiclass classification</span></strong><span> adds complexity, as may be expected, but relies on similar mechanics. We will have the opportunity to explore this area later in the course.</span></p><br><h2><a name="7-tree-based-models-for-classification-and-regression" class="md-header-anchor"></a><span>7. Tree-based Models for Classification and Regression</span></h2><h3><a name="the-decision-tree" class="md-header-anchor"></a><span>The Decision Tree</span></h3><p><span>Tree-based models provide an alternative means to approach both branches of supervised learning. Such models are often grouped under the heading of Classification and Regression Tree (CART) analysis. All of these models are based on the humble, human-interpretable decision tree:</span></p><p align="center">
    <img src="https://capsseminar.github.io/static/notes/lecture2/tree_example.PNG" max-height="350" alt="Example decision tree"></p><p align="center">
    <i>Figure 2: Graphical representation of an example decision tree for a classification problem with six features and possible target variable values X, Y, and Z. Note that decision trees may be represented in other ways, but the core concepts remain the same.
</i></p><p><span>The basic intuition of a decision tree is similar to the game &quot;twenty questions&quot;, in which a series of yes/no questions leads to a prediction of the hidden answer. The decision tree formalizes the process of questioning into a reproducible analysis of specific features, while allowing for a far broader range of questions.</span></p><p><span>In the figure above, the features assessed are represented as transparent boxes known as </span><strong><span>nodes</span></strong><span>, with possible values represented as lines and known as </span><strong><span>branches</span></strong><span>. The potential outcomes - which could be the results of either classification or regression - are represented as blue boxes and known as </span><strong><span>leaves</span></strong><span>. As the figure suggests, nodes may have varying numbers of branches, and leaves may be reached at any level. This flexibility allows considerable model adaptation.</span></p><p><span>In a regression problem, where the features are continuous variables, a node can assess whether a feature value is above or below a certain value. A sophisticated classification model may assess both categorical and numerical variables in different nodes.</span></p><h3><a name="training-a-tree" class="md-header-anchor"></a><span>Training a Tree</span></h3><p><span>Numerous algorithms exist for decision tree learning; in general, however, the algorithms perform the same basic function: to identify the best splits for each node, working from the top down. The general procedure is:</span></p><ol start='' ><li><span>Test which feature best splits the labeled data</span></li><li><span>Set the identified feature as the first node</span></li><li><span>Test which feature best splits each now-segregated set of labeled data </span></li><li><span>Establish the identified features as the next respective nodes</span></li></ol><p><span>And so forth, until the measure used to test how well a feature splits the data falls below a specified threshold or reaches zero (indicating no further benefit to splitting). Where continuous variables are employed, the model also determines threshold values that best split the data.</span></p><h4><a name="key-algorithms" class="md-header-anchor"></a><span>Key Algorithms</span></h4><p><span>Three important algorithms are discussed below, all of which identify splits (for further discussion, see </span><a href='https://medium.com/greyatom/decision-trees-a-simple-way-to-visualize-a-decision-dc506a403aeb##targetText=A%20decision%20tree%20is%20a,only%20contains%20conditional%20control%20statements'><span>2</span></a><span>). The following presentations assume binary splits, i.e. two branches per node, although many algorithms can be adapted for other cases.</span></p><h5><a name="information-gain" class="md-header-anchor"></a><span>Information Gain</span></h5><ul><li><p><span>This algorithm determines which features yield the most </span><em><span>information</span></em><span>, using a concept called </span><strong><span>entropy</span></strong><span>, which can be expressed as:</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n281" cid="n281" mdtype="math_block"><div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-6-Frame" tabindex="-1" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="27.476ex" height="7.13ex" viewBox="0 -1660.6 11830.1 3069.8" role="img" focusable="false" style="vertical-align: -3.273ex; max-width: 100%;"><defs><path stroke-width="0" id="E6-MJMATHI-45" d="M492 213Q472 213 472 226Q472 230 477 250T482 285Q482 316 461 323T364 330H312Q311 328 277 192T243 52Q243 48 254 48T334 46Q428 46 458 48T518 61Q567 77 599 117T670 248Q680 270 683 272Q690 274 698 274Q718 274 718 261Q613 7 608 2Q605 0 322 0H133Q31 0 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H757Q764 676 764 669Q764 664 751 557T737 447Q735 440 717 440H705Q698 445 698 453L701 476Q704 500 704 528Q704 558 697 578T678 609T643 625T596 632T532 634H485Q397 633 392 631Q388 629 386 622Q385 619 355 499T324 377Q347 376 372 376H398Q464 376 489 391T534 472Q538 488 540 490T557 493Q562 493 565 493T570 492T572 491T574 487T577 483L544 351Q511 218 508 216Q505 213 492 213Z"></path><path stroke-width="0" id="E6-MJMATHI-6E" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path><path stroke-width="0" id="E6-MJMATHI-74" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path><path stroke-width="0" id="E6-MJMATHI-72" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path stroke-width="0" id="E6-MJMATHI-6F" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path><path stroke-width="0" id="E6-MJMATHI-70" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path><path stroke-width="0" id="E6-MJMATHI-79" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path stroke-width="0" id="E6-MJMAIN-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path stroke-width="0" id="E6-MJMAIN-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path stroke-width="0" id="E6-MJSZ2-2211" d="M60 948Q63 950 665 950H1267L1325 815Q1384 677 1388 669H1348L1341 683Q1320 724 1285 761Q1235 809 1174 838T1033 881T882 898T699 902H574H543H251L259 891Q722 258 724 252Q725 250 724 246Q721 243 460 -56L196 -356Q196 -357 407 -357Q459 -357 548 -357T676 -358Q812 -358 896 -353T1063 -332T1204 -283T1307 -196Q1328 -170 1348 -124H1388Q1388 -125 1381 -145T1356 -210T1325 -294L1267 -449L666 -450Q64 -450 61 -448Q55 -446 55 -439Q55 -437 57 -433L590 177Q590 178 557 222T452 366T322 544L56 909L55 924Q55 945 60 948Z"></path><path stroke-width="0" id="E6-MJMATHI-6A" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path><path stroke-width="0" id="E6-MJMAIN-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path stroke-width="0" id="E6-MJMATHI-6C" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path><path stroke-width="0" id="E6-MJMATHI-67" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path><path stroke-width="0" id="E6-MJMAIN-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="#E6-MJMATHI-45" x="0" y="0"></use><use xlink:href="#E6-MJMATHI-6E" x="764" y="0"></use><use xlink:href="#E6-MJMATHI-74" x="1364" y="0"></use><use xlink:href="#E6-MJMATHI-72" x="1725" y="0"></use><use xlink:href="#E6-MJMATHI-6F" x="2176" y="0"></use><use xlink:href="#E6-MJMATHI-70" x="2661" y="0"></use><use xlink:href="#E6-MJMATHI-79" x="3164" y="0"></use><use xlink:href="#E6-MJMAIN-3D" x="3938" y="0"></use><use xlink:href="#E6-MJMAIN-2212" x="4994" y="0"></use><g transform="translate(5939,0)"><use xlink:href="#E6-MJSZ2-2211" x="0" y="0"></use><g transform="translate(124,-1088)"><use transform="scale(0.707)" xlink:href="#E6-MJMATHI-6A" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="#E6-MJMAIN-3D" x="412" y="0"></use><use transform="scale(0.707)" xlink:href="#E6-MJMAIN-31" x="1189" y="0"></use></g><use transform="scale(0.707)" xlink:href="#E6-MJMATHI-6E" x="721" y="1626"></use></g><use xlink:href="#E6-MJMAIN-2212" x="7549" y="0"></use><g transform="translate(8327,0)"><use xlink:href="#E6-MJMATHI-70" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="#E6-MJMATHI-6A" x="711" y="-213"></use></g><use xlink:href="#E6-MJMATHI-6C" x="9222" y="0"></use><use xlink:href="#E6-MJMATHI-6F" x="9520" y="0"></use><g transform="translate(10005,0)"><use xlink:href="#E6-MJMATHI-67" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="#E6-MJMAIN-32" x="674" y="-213"></use></g><g transform="translate(10935,0)"><use xlink:href="#E6-MJMATHI-70" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="#E6-MJMATHI-6A" x="711" y="-213"></use></g></g></svg></span></div><script type="math/tex; mode=display" id="MathJax-Element-6">Entropy = -\sum_{j=1}^n -p_j log_2p_j</script></div></div><p><span>where </span><span class="MathJax_SVG" tabindex="-1" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="5.08ex" height="2.577ex" viewBox="0 -806.1 2187 1109.7" role="img" focusable="false" style="vertical-align: -0.705ex;"><defs><path stroke-width="0" id="E26-MJMATHI-45" d="M492 213Q472 213 472 226Q472 230 477 250T482 285Q482 316 461 323T364 330H312Q311 328 277 192T243 52Q243 48 254 48T334 46Q428 46 458 48T518 61Q567 77 599 117T670 248Q680 270 683 272Q690 274 698 274Q718 274 718 261Q613 7 608 2Q605 0 322 0H133Q31 0 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H757Q764 676 764 669Q764 664 751 557T737 447Q735 440 717 440H705Q698 445 698 453L701 476Q704 500 704 528Q704 558 697 578T678 609T643 625T596 632T532 634H485Q397 633 392 631Q388 629 386 622Q385 619 355 499T324 377Q347 376 372 376H398Q464 376 489 391T534 472Q538 488 540 490T557 493Q562 493 565 493T570 492T572 491T574 487T577 483L544 351Q511 218 508 216Q505 213 492 213Z"></path><path stroke-width="0" id="E26-MJMAIN-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path stroke-width="0" id="E26-MJMATHI-53" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"></path><path stroke-width="0" id="E26-MJMAIN-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="#E26-MJMATHI-45" x="0" y="0"></use><use xlink:href="#E26-MJMAIN-28" x="764" y="0"></use><use xlink:href="#E26-MJMATHI-53" x="1153" y="0"></use><use xlink:href="#E26-MJMAIN-29" x="1798" y="0"></use></g></svg></span><script type="math/tex">E(S)</script><span> is entropy, </span><span class="MathJax_SVG" tabindex="-1" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.259ex" height="1.76ex" viewBox="-39 -504.6 542 757.9" role="img" focusable="false" style="vertical-align: -0.588ex; margin-left: -0.091ex;"><defs><path stroke-width="0" id="E27-MJMATHI-70" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="#E27-MJMATHI-70" x="0" y="0"></use></g></svg></span><script type="math/tex">p</script><span> is the probability of outcome 1 and q is the probability of outcome 2.</span></p></li><li><p><span>Information gain is equal to </span><span class="MathJax_SVG" tabindex="-1" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="9.08ex" height="2.577ex" viewBox="0 -806.1 3909.4 1109.7" role="img" focusable="false" style="vertical-align: -0.705ex;"><defs><path stroke-width="0" id="E28-MJMAIN-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path stroke-width="0" id="E28-MJMAIN-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path stroke-width="0" id="E28-MJMATHI-45" d="M492 213Q472 213 472 226Q472 230 477 250T482 285Q482 316 461 323T364 330H312Q311 328 277 192T243 52Q243 48 254 48T334 46Q428 46 458 48T518 61Q567 77 599 117T670 248Q680 270 683 272Q690 274 698 274Q718 274 718 261Q613 7 608 2Q605 0 322 0H133Q31 0 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H757Q764 676 764 669Q764 664 751 557T737 447Q735 440 717 440H705Q698 445 698 453L701 476Q704 500 704 528Q704 558 697 578T678 609T643 625T596 632T532 634H485Q397 633 392 631Q388 629 386 622Q385 619 355 499T324 377Q347 376 372 376H398Q464 376 489 391T534 472Q538 488 540 490T557 493Q562 493 565 493T570 492T572 491T574 487T577 483L544 351Q511 218 508 216Q505 213 492 213Z"></path><path stroke-width="0" id="E28-MJMAIN-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path stroke-width="0" id="E28-MJMATHI-53" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"></path><path stroke-width="0" id="E28-MJMAIN-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="#E28-MJMAIN-31" x="0" y="0"></use><use xlink:href="#E28-MJMAIN-2212" x="722" y="0"></use><use xlink:href="#E28-MJMATHI-45" x="1722" y="0"></use><use xlink:href="#E28-MJMAIN-28" x="2486" y="0"></use><use xlink:href="#E28-MJMATHI-53" x="2875" y="0"></use><use xlink:href="#E28-MJMAIN-29" x="3520" y="0"></use></g></svg></span><script type="math/tex">1 - E(S)</script><span>. The algorithms identifies the split that minimizes entropy, which indicates maximal information gained.</span></p></li></ul><h5><a name="gini-index" class="md-header-anchor"></a><span>Gini Index</span></h5><ul><li><p><span>The Gini index (also known as Gini impurity), measures the probability that a random classification will be incorrect. If </span><em><span>all</span></em><span> data belong to the same category (e.g. in the loan case, if the dataset contains zero defaults), then the Gini index returns a value of 0; this category is then known as &quot;pure&quot;. A completely random distribution of data across classes results in a 1 - completely &quot;impure&quot;.</span></p></li><li><p><span>The Gini index is largely famous for its use in measuring income inequality.</span></p></li><li><p><span>The most basic expression of the Gini index for decision tree-based classification is:</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n293" cid="n293" mdtype="math_block"><div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-7-Frame" tabindex="-1" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="17.882ex" height="6.78ex" viewBox="0 -1660.6 7699.2 2919" role="img" focusable="false" style="vertical-align: -2.923ex; max-width: 100%;"><defs><path stroke-width="0" id="E36-MJMATHI-47" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q492 659 471 656T418 643T357 615T294 567T236 496T189 394T158 260Q156 242 156 221Q156 173 170 136T206 79T256 45T308 28T353 24Q407 24 452 47T514 106Q517 114 529 161T541 214Q541 222 528 224T468 227H431Q425 233 425 235T427 254Q431 267 437 273H454Q494 271 594 271Q634 271 659 271T695 272T707 272Q721 272 721 263Q721 261 719 249Q714 230 709 228Q706 227 694 227Q674 227 653 224Q646 221 643 215T629 164Q620 131 614 108Q589 6 586 3Q584 1 581 1Q571 1 553 21T530 52Q530 53 528 52T522 47Q448 -22 322 -22Q201 -22 126 55T50 252Z"></path><path stroke-width="0" id="E36-MJMATHI-69" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path stroke-width="0" id="E36-MJMATHI-6E" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path><path stroke-width="0" id="E36-MJMAIN-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path stroke-width="0" id="E36-MJMAIN-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path stroke-width="0" id="E36-MJMAIN-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path stroke-width="0" id="E36-MJSZ2-2211" d="M60 948Q63 950 665 950H1267L1325 815Q1384 677 1388 669H1348L1341 683Q1320 724 1285 761Q1235 809 1174 838T1033 881T882 898T699 902H574H543H251L259 891Q722 258 724 252Q725 250 724 246Q721 243 460 -56L196 -356Q196 -357 407 -357Q459 -357 548 -357T676 -358Q812 -358 896 -353T1063 -332T1204 -283T1307 -196Q1328 -170 1348 -124H1388Q1388 -125 1381 -145T1356 -210T1325 -294L1267 -449L666 -450Q64 -450 61 -448Q55 -446 55 -439Q55 -437 57 -433L590 177Q590 178 557 222T452 366T322 544L56 909L55 924Q55 945 60 948Z"></path><path stroke-width="0" id="E36-MJMATHI-70" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path><path stroke-width="0" id="E36-MJMAIN-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="#E36-MJMATHI-47" x="0" y="0"></use><use xlink:href="#E36-MJMATHI-69" x="786" y="0"></use><use xlink:href="#E36-MJMATHI-6E" x="1131" y="0"></use><use xlink:href="#E36-MJMATHI-69" x="1731" y="0"></use><use xlink:href="#E36-MJMAIN-3D" x="2353" y="0"></use><use xlink:href="#E36-MJMAIN-31" x="3409" y="0"></use><use xlink:href="#E36-MJMAIN-2212" x="4131" y="0"></use><g transform="translate(5132,0)"><use xlink:href="#E36-MJSZ2-2211" x="0" y="0"></use><g transform="translate(148,-1088)"><use transform="scale(0.707)" xlink:href="#E36-MJMATHI-69" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="#E36-MJMAIN-3D" x="345" y="0"></use><use transform="scale(0.707)" xlink:href="#E36-MJMAIN-31" x="1123" y="0"></use></g><use transform="scale(0.707)" xlink:href="#E36-MJMATHI-6E" x="721" y="1626"></use></g><g transform="translate(6742,0)"><use xlink:href="#E36-MJMATHI-70" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="#E36-MJMAIN-32" x="711" y="487"></use><use transform="scale(0.707)" xlink:href="#E36-MJMATHI-69" x="711" y="-429"></use></g></g></svg></span></div><script type="math/tex; mode=display" id="MathJax-Element-7">Gini = 1 - \sum_{i=1}^n p_i^2</script></div></div><p><span>where </span><span class="MathJax_SVG" tabindex="-1" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.394ex" height="1.41ex" viewBox="0 -504.6 600 607.1" role="img" focusable="false" style="vertical-align: -0.238ex;"><defs><path stroke-width="0" id="E33-MJMATHI-6E" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="#E33-MJMATHI-6E" x="0" y="0"></use></g></svg></span><script type="math/tex">n</script><span> is the number of categories and </span><span class="MathJax_SVG" tabindex="-1" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="2.058ex" height="1.76ex" viewBox="-39 -504.6 886 757.9" role="img" focusable="false" style="vertical-align: -0.588ex; margin-left: -0.091ex;"><defs><path stroke-width="0" id="E30-MJMATHI-70" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path><path stroke-width="0" id="E30-MJMATHI-69" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="#E30-MJMATHI-70" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="#E30-MJMATHI-69" x="711" y="-213"></use></g></svg></span><script type="math/tex">p_i</script><span> is the probability of a data point&#39;s classification in the given category.</span></p></li></ul><h5><a name="chi-square" class="md-header-anchor"></a><span>Chi-Square</span></h5><ul><li><p><span>Chi-square measures the statistical significance of a given split, allowing the model to choose the maximally significant split.</span></p></li><li><p><span>The chi-square formula is, for a given observation </span><span class="MathJax_SVG" tabindex="-1" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.772ex" height="2.11ex" viewBox="0 -806.1 763 908.7" role="img" focusable="false" style="vertical-align: -0.238ex;"><defs><path stroke-width="0" id="E31-MJMATHI-4F" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="#E31-MJMATHI-4F" x="0" y="0"></use></g></svg></span><script type="math/tex">O</script><span> and a given expected value </span><span class="MathJax_SVG" tabindex="-1" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.774ex" height="1.877ex" viewBox="0 -755.9 764 808.1" role="img" focusable="false" style="vertical-align: -0.121ex;"><defs><path stroke-width="0" id="E32-MJMATHI-45" d="M492 213Q472 213 472 226Q472 230 477 250T482 285Q482 316 461 323T364 330H312Q311 328 277 192T243 52Q243 48 254 48T334 46Q428 46 458 48T518 61Q567 77 599 117T670 248Q680 270 683 272Q690 274 698 274Q718 274 718 261Q613 7 608 2Q605 0 322 0H133Q31 0 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H757Q764 676 764 669Q764 664 751 557T737 447Q735 440 717 440H705Q698 445 698 453L701 476Q704 500 704 528Q704 558 697 578T678 609T643 625T596 632T532 634H485Q397 633 392 631Q388 629 386 622Q385 619 355 499T324 377Q347 376 372 376H398Q464 376 489 391T534 472Q538 488 540 490T557 493Q562 493 565 493T570 492T572 491T574 487T577 483L544 351Q511 218 508 216Q505 213 492 213Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="#E32-MJMATHI-45" x="0" y="0"></use></g></svg></span><script type="math/tex">E</script><span>:</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n301" cid="n301" mdtype="math_block"><div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-8-Frame" tabindex="-1" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="20.965ex" height="6.78ex" viewBox="0 -1660.6 9026.7 2919" role="img" focusable="false" style="vertical-align: -2.923ex; max-width: 100%;"><defs><path stroke-width="0" id="E37-MJMATHI-3C7" d="M576 -125Q576 -147 547 -175T487 -204H476Q394 -204 363 -157Q334 -114 293 26L284 59Q283 58 248 19T170 -66T92 -151T53 -191Q49 -194 43 -194Q36 -194 31 -189T25 -177T38 -154T151 -30L272 102L265 131Q189 405 135 405Q104 405 87 358Q86 351 68 351Q48 351 48 361Q48 369 56 386T89 423T148 442Q224 442 258 400Q276 375 297 320T330 222L341 180Q344 180 455 303T573 429Q579 431 582 431Q600 431 600 414Q600 407 587 392T477 270Q356 138 353 134L362 102Q392 -10 428 -89T490 -168Q504 -168 517 -156T536 -126Q539 -116 543 -115T557 -114T571 -115Q576 -118 576 -125Z"></path><path stroke-width="0" id="E37-MJMAIN-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path stroke-width="0" id="E37-MJMAIN-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path stroke-width="0" id="E37-MJSZ2-2211" d="M60 948Q63 950 665 950H1267L1325 815Q1384 677 1388 669H1348L1341 683Q1320 724 1285 761Q1235 809 1174 838T1033 881T882 898T699 902H574H543H251L259 891Q722 258 724 252Q725 250 724 246Q721 243 460 -56L196 -356Q196 -357 407 -357Q459 -357 548 -357T676 -358Q812 -358 896 -353T1063 -332T1204 -283T1307 -196Q1328 -170 1348 -124H1388Q1388 -125 1381 -145T1356 -210T1325 -294L1267 -449L666 -450Q64 -450 61 -448Q55 -446 55 -439Q55 -437 57 -433L590 177Q590 178 557 222T452 366T322 544L56 909L55 924Q55 945 60 948Z"></path><path stroke-width="0" id="E37-MJMATHI-69" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path stroke-width="0" id="E37-MJMAIN-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path stroke-width="0" id="E37-MJMATHI-6E" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path><path stroke-width="0" id="E37-MJMAIN-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path stroke-width="0" id="E37-MJMATHI-4F" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path><path stroke-width="0" id="E37-MJMAIN-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path stroke-width="0" id="E37-MJMATHI-45" d="M492 213Q472 213 472 226Q472 230 477 250T482 285Q482 316 461 323T364 330H312Q311 328 277 192T243 52Q243 48 254 48T334 46Q428 46 458 48T518 61Q567 77 599 117T670 248Q680 270 683 272Q690 274 698 274Q718 274 718 261Q613 7 608 2Q605 0 322 0H133Q31 0 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H757Q764 676 764 669Q764 664 751 557T737 447Q735 440 717 440H705Q698 445 698 453L701 476Q704 500 704 528Q704 558 697 578T678 609T643 625T596 632T532 634H485Q397 633 392 631Q388 629 386 622Q385 619 355 499T324 377Q347 376 372 376H398Q464 376 489 391T534 472Q538 488 540 490T557 493Q562 493 565 493T570 492T572 491T574 487T577 483L544 351Q511 218 508 216Q505 213 492 213Z"></path><path stroke-width="0" id="E37-MJMAIN-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="#E37-MJMATHI-3C7" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="#E37-MJMAIN-32" x="885" y="583"></use><use xlink:href="#E37-MJMAIN-3D" x="1357" y="0"></use><g transform="translate(2413,0)"><use xlink:href="#E37-MJSZ2-2211" x="0" y="0"></use><g transform="translate(148,-1088)"><use transform="scale(0.707)" xlink:href="#E37-MJMATHI-69" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="#E37-MJMAIN-3D" x="345" y="0"></use><use transform="scale(0.707)" xlink:href="#E37-MJMAIN-31" x="1123" y="0"></use></g><use transform="scale(0.707)" xlink:href="#E37-MJMATHI-6E" x="721" y="1626"></use></g><g transform="translate(3857,0)"><g transform="translate(286,0)"><rect stroke="none" width="4762" height="60" x="0" y="220"></rect><g transform="translate(60,715)"><use xlink:href="#E37-MJMAIN-28" x="0" y="0"></use><g transform="translate(389,0)"><use xlink:href="#E37-MJMATHI-4F" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="#E37-MJMATHI-69" x="1079" y="-213"></use></g><use xlink:href="#E37-MJMAIN-2212" x="1718" y="0"></use><g transform="translate(2718,0)"><use xlink:href="#E37-MJMATHI-45" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="#E37-MJMATHI-69" x="1043" y="-213"></use></g><g transform="translate(3800,0)"><use xlink:href="#E37-MJMAIN-29" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="#E37-MJMAIN-32" x="550" y="513"></use></g></g><g transform="translate(1840,-686)"><use xlink:href="#E37-MJMATHI-45" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="#E37-MJMATHI-69" x="1043" y="-213"></use></g></g></g></g></svg></span></div><script type="math/tex; mode=display" id="MathJax-Element-8">\chi^2 = \sum_{i=1}^n\frac{(O_i - E_i)^2}{E_i}</script></div></div></li></ul><h5><a name="notes" class="md-header-anchor"></a><span>Notes</span></h5><p><span>Other algorithms include variance reduction, standard deviation reduction, and the like. As the names imply, they select splits that minimize variance, standard deviation, etc. between the parent and child nodes of each split.</span></p><p><strong><span>Complete application of these techniques is far more involved that the base formulas above may imply</span></strong><span>; gladly, the data transformations involved are packaged into machine learning libraries for efficient development. The base formulas presented do, however, shed light on the central goal of each given algorithm.</span></p><h4><a name="overfitting" class="md-header-anchor"></a><span>Overfitting</span></h4><p><span>There are, in general, two approaches to handle overfitting in a single tree-based model.</span></p><ol start='' ><li><span>Allow the tree to grow to completion, allowing overfitting, then </span><strong><span>prune</span></strong><span> the tree by identifying and removing features that have a very low impact on the outcome.</span></li><li><span>Speciify a maximum tree depth, and </span><strong><span>halt</span></strong><span> tree growth before the point at which overfitting is expected.</span></li></ol><p><span>While consciously allowing a model to overfit before correcting it may seem counter-intuitive, the alternative makes clear a difficult challenge: how deep </span><em><span>should</span></em><span> it be allowed to grow? While the decision is unlikely to be a complete guess, it is also far from an exact science. Pre-specified cutoffs risk both halting a tree </span><em><span>before</span></em><span> optimal depth is reached, and halting it </span><em><span>after</span></em><span> optimal depth is reached, in which case one returns to pruning the tree (albeit likely less so than if no cutoff is specified).</span></p><p><span>How does one prune a tree? To prune, developers will use the selected statistical measure to determine the contribution of each feature. While pruning, the model is tested repeatedly to measure the accuracy or other metric of interest, until overfitting is corrected and the desired performance is achieved.</span></p><p><span>How does one halt a tree before overfitting? There are multiple options. Most simply, an engineer can simply specify that a tree shall have no more than X levels or splits; the algorithm should cease if the specified depth is reached. As a more technical approach, they might also decide a threshold of minimum improvement for the algorithm to cease. As we discussed above, the algorithms can be allowed to proceed in making splits until the splits cease to yield improvements, where the improvements in question are dictated by the selected algorithm (e.g. when entropy is no longer decreased, for the case of information gain). An engineer can specify a higher threshold at which &quot;improvement&quot; is no longer sufficient to continue splitting, in principle ending the process before overfitting occurs.</span></p><h4><a name="trial-and-error" class="md-header-anchor"></a><span>Trial and Error</span></h4><p><span>At the end of the day, models require iteration. While there are tools to hasten the process, there is no crystal ball to reach an ideal system from the first attempt. Instead, best first attempts are put in place, and then models are </span><em><span>evaluated</span></em><span>, reconfigured, and </span><em><span>evaluated again</span></em><span>, until satisfactory results are achieved. Or, if resources are running low, a suboptimal model might be let free as &quot;good enough&quot;... hence the importance of ensuring project leaders understand what goes into optimization, and making sure the assessment of &quot;good enough&quot; truly is, in fact, good enough.</span></p><h3><a name="beyond-single-trees" class="md-header-anchor"></a><span>Beyond Single Trees</span></h3><p><span>As mentioned at the start of the section, decision trees are the foundations of tree-based models. Building on these are </span><strong><span>ensemble models</span></strong><span>, such as the </span><strong><span>random forest model</span></strong><span>, which in effect averages results across multiple decision trees, thereby smoothing potential overfitting by each individual model. Such models lay beyond the scope of what can reasonably be covered here, but with the given foundations you will be prepared to research them moving forward.</span></p><h3><a name="example-decision-tree-code" class="md-header-anchor"></a><span>Example Decision Tree Code</span></h3><br><h2><a name="8-model-evaluation" class="md-header-anchor"></a><span>8. Model Evaluation</span></h2><p><span>Model evaluation is tailored to both the model type and the specific goals of the model use case. We will discuss the foundational ideas, with the usual caveat that this short introduction cannot prove complete coverage. However, it does include the key foundations for working with the full range of metrics one might encounter in the future.</span></p><h3><a name="regression" class="md-header-anchor"></a><span>Regression</span></h3><p><span>Regression models are evaluated by variations on a theme: how </span><em><span>close</span></em><span> are the predicted results to the values? Foundational techniques include R</span><sup><span>2</span></sup><span> and adjusted R</span><sup><span>2</span></sup><span>. The most important, more robust methods, however, are </span><strong><span>mean absolute error (MAE)</span></strong><span> and </span><strong><span>root mean square error (RMSE)</span></strong><span>.</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n325" cid="n325" mdtype="math_block"><div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-9-Frame" tabindex="-1" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="21.611ex" height="6.78ex" viewBox="0 -1660.6 9304.8 2919" role="img" focusable="false" style="vertical-align: -2.923ex; max-width: 100%;"><defs><path stroke-width="0" id="E38-MJMATHI-4D" d="M289 629Q289 635 232 637Q208 637 201 638T194 648Q194 649 196 659Q197 662 198 666T199 671T201 676T203 679T207 681T212 683T220 683T232 684Q238 684 262 684T307 683Q386 683 398 683T414 678Q415 674 451 396L487 117L510 154Q534 190 574 254T662 394Q837 673 839 675Q840 676 842 678T846 681L852 683H948Q965 683 988 683T1017 684Q1051 684 1051 673Q1051 668 1048 656T1045 643Q1041 637 1008 637Q968 636 957 634T939 623Q936 618 867 340T797 59Q797 55 798 54T805 50T822 48T855 46H886Q892 37 892 35Q892 19 885 5Q880 0 869 0Q864 0 828 1T736 2Q675 2 644 2T609 1Q592 1 592 11Q592 13 594 25Q598 41 602 43T625 46Q652 46 685 49Q699 52 704 61Q706 65 742 207T813 490T848 631L654 322Q458 10 453 5Q451 4 449 3Q444 0 433 0Q418 0 415 7Q413 11 374 317L335 624L267 354Q200 88 200 79Q206 46 272 46H282Q288 41 289 37T286 19Q282 3 278 1Q274 0 267 0Q265 0 255 0T221 1T157 2Q127 2 95 1T58 0Q43 0 39 2T35 11Q35 13 38 25T43 40Q45 46 65 46Q135 46 154 86Q158 92 223 354T289 629Z"></path><path stroke-width="0" id="E38-MJMATHI-41" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path><path stroke-width="0" id="E38-MJMATHI-45" d="M492 213Q472 213 472 226Q472 230 477 250T482 285Q482 316 461 323T364 330H312Q311 328 277 192T243 52Q243 48 254 48T334 46Q428 46 458 48T518 61Q567 77 599 117T670 248Q680 270 683 272Q690 274 698 274Q718 274 718 261Q613 7 608 2Q605 0 322 0H133Q31 0 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H757Q764 676 764 669Q764 664 751 557T737 447Q735 440 717 440H705Q698 445 698 453L701 476Q704 500 704 528Q704 558 697 578T678 609T643 625T596 632T532 634H485Q397 633 392 631Q388 629 386 622Q385 619 355 499T324 377Q347 376 372 376H398Q464 376 489 391T534 472Q538 488 540 490T557 493Q562 493 565 493T570 492T572 491T574 487T577 483L544 351Q511 218 508 216Q505 213 492 213Z"></path><path stroke-width="0" id="E38-MJMAIN-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path stroke-width="0" id="E38-MJMAIN-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path stroke-width="0" id="E38-MJMATHI-6E" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path><path stroke-width="0" id="E38-MJSZ2-2211" d="M60 948Q63 950 665 950H1267L1325 815Q1384 677 1388 669H1348L1341 683Q1320 724 1285 761Q1235 809 1174 838T1033 881T882 898T699 902H574H543H251L259 891Q722 258 724 252Q725 250 724 246Q721 243 460 -56L196 -356Q196 -357 407 -357Q459 -357 548 -357T676 -358Q812 -358 896 -353T1063 -332T1204 -283T1307 -196Q1328 -170 1348 -124H1388Q1388 -125 1381 -145T1356 -210T1325 -294L1267 -449L666 -450Q64 -450 61 -448Q55 -446 55 -439Q55 -437 57 -433L590 177Q590 178 557 222T452 366T322 544L56 909L55 924Q55 945 60 948Z"></path><path stroke-width="0" id="E38-MJMATHI-69" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path stroke-width="0" id="E38-MJMAIN-7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path><path stroke-width="0" id="E38-MJMATHI-79" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path stroke-width="0" id="E38-MJMAIN-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path stroke-width="0" id="E38-MJMAIN-5E" d="M112 560L249 694L257 686Q387 562 387 560L361 531Q359 532 303 581L250 627L195 580Q182 569 169 557T148 538L140 532Q138 530 125 546L112 560Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="#E38-MJMATHI-4D" x="0" y="0"></use><use xlink:href="#E38-MJMATHI-41" x="1051" y="0"></use><use xlink:href="#E38-MJMATHI-45" x="1801" y="0"></use><use xlink:href="#E38-MJMAIN-3D" x="2842" y="0"></use><g transform="translate(3620,0)"><g transform="translate(397,0)"><rect stroke="none" width="720" height="60" x="0" y="220"></rect><use xlink:href="#E38-MJMAIN-31" x="110" y="676"></use><use xlink:href="#E38-MJMATHI-6E" x="60" y="-686"></use></g></g><g transform="translate(5025,0)"><use xlink:href="#E38-MJSZ2-2211" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="#E38-MJMATHI-69" x="848" y="-1534"></use><use transform="scale(0.707)" xlink:href="#E38-MJMATHI-6E" x="721" y="1626"></use></g><use xlink:href="#E38-MJMAIN-7C" x="6469" y="0"></use><use xlink:href="#E38-MJMATHI-79" x="6747" y="0"></use><use xlink:href="#E38-MJMAIN-2212" x="7466" y="0"></use><g transform="translate(8466,0)"><use xlink:href="#E38-MJMATHI-79" x="1" y="0"></use><use xlink:href="#E38-MJMAIN-5E" x="60" y="-13"></use></g><use xlink:href="#E38-MJMAIN-7C" x="9026" y="0"></use></g></svg></span></div><script type="math/tex; mode=display" id="MathJax-Element-9">MAE = \frac{1}{n} \sum_i^n \lvert y - \hat{y} \rvert</script></div></div><p><span>where </span><span class="MathJax_SVG" tabindex="-1" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.394ex" height="1.41ex" viewBox="0 -504.6 600 607.1" role="img" focusable="false" style="vertical-align: -0.238ex;"><defs><path stroke-width="0" id="E33-MJMATHI-6E" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="#E33-MJMATHI-6E" x="0" y="0"></use></g></svg></span><script type="math/tex">n</script><span> is the number of data points, </span><span class="MathJax_SVG" tabindex="-1" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.154ex" height="1.877ex" viewBox="0 -504.6 497 808.1" role="img" focusable="false" style="vertical-align: -0.705ex;"><defs><path stroke-width="0" id="E34-MJMATHI-79" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="#E34-MJMATHI-79" x="0" y="0"></use></g></svg></span><script type="math/tex">y</script><span> is a true value, and </span><span class="MathJax_SVG" tabindex="-1" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.301ex" height="2.461ex" viewBox="0 -755.9 560.2 1059.4" role="img" focusable="false" style="vertical-align: -0.705ex;"><defs><path stroke-width="0" id="E35-MJMATHI-79" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path stroke-width="0" id="E35-MJMAIN-5E" d="M112 560L249 694L257 686Q387 562 387 560L361 531Q359 532 303 581L250 627L195 580Q182 569 169 557T148 538L140 532Q138 530 125 546L112 560Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="#E35-MJMATHI-79" x="1" y="0"></use><use xlink:href="#E35-MJMAIN-5E" x="60" y="-13"></use></g></svg></span><script type="math/tex">\hat{y}</script><span> is a predicted value.</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n327" cid="n327" mdtype="math_block"><div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-10-Frame" tabindex="-1" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="27.022ex" height="7.48ex" viewBox="0 -1811.3 11634.4 3220.6" role="img" focusable="false" style="vertical-align: -3.273ex; max-width: 100%;"><defs><path stroke-width="0" id="E39-MJMATHI-52" d="M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z"></path><path stroke-width="0" id="E39-MJMATHI-4D" d="M289 629Q289 635 232 637Q208 637 201 638T194 648Q194 649 196 659Q197 662 198 666T199 671T201 676T203 679T207 681T212 683T220 683T232 684Q238 684 262 684T307 683Q386 683 398 683T414 678Q415 674 451 396L487 117L510 154Q534 190 574 254T662 394Q837 673 839 675Q840 676 842 678T846 681L852 683H948Q965 683 988 683T1017 684Q1051 684 1051 673Q1051 668 1048 656T1045 643Q1041 637 1008 637Q968 636 957 634T939 623Q936 618 867 340T797 59Q797 55 798 54T805 50T822 48T855 46H886Q892 37 892 35Q892 19 885 5Q880 0 869 0Q864 0 828 1T736 2Q675 2 644 2T609 1Q592 1 592 11Q592 13 594 25Q598 41 602 43T625 46Q652 46 685 49Q699 52 704 61Q706 65 742 207T813 490T848 631L654 322Q458 10 453 5Q451 4 449 3Q444 0 433 0Q418 0 415 7Q413 11 374 317L335 624L267 354Q200 88 200 79Q206 46 272 46H282Q288 41 289 37T286 19Q282 3 278 1Q274 0 267 0Q265 0 255 0T221 1T157 2Q127 2 95 1T58 0Q43 0 39 2T35 11Q35 13 38 25T43 40Q45 46 65 46Q135 46 154 86Q158 92 223 354T289 629Z"></path><path stroke-width="0" id="E39-MJMATHI-53" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"></path><path stroke-width="0" id="E39-MJMATHI-45" d="M492 213Q472 213 472 226Q472 230 477 250T482 285Q482 316 461 323T364 330H312Q311 328 277 192T243 52Q243 48 254 48T334 46Q428 46 458 48T518 61Q567 77 599 117T670 248Q680 270 683 272Q690 274 698 274Q718 274 718 261Q613 7 608 2Q605 0 322 0H133Q31 0 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H757Q764 676 764 669Q764 664 751 557T737 447Q735 440 717 440H705Q698 445 698 453L701 476Q704 500 704 528Q704 558 697 578T678 609T643 625T596 632T532 634H485Q397 633 392 631Q388 629 386 622Q385 619 355 499T324 377Q347 376 372 376H398Q464 376 489 391T534 472Q538 488 540 490T557 493Q562 493 565 493T570 492T572 491T574 487T577 483L544 351Q511 218 508 216Q505 213 492 213Z"></path><path stroke-width="0" id="E39-MJMAIN-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path stroke-width="0" id="E39-MJMAIN-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path stroke-width="0" id="E39-MJMATHI-6E" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path><path stroke-width="0" id="E39-MJSZ2-2211" d="M60 948Q63 950 665 950H1267L1325 815Q1384 677 1388 669H1348L1341 683Q1320 724 1285 761Q1235 809 1174 838T1033 881T882 898T699 902H574H543H251L259 891Q722 258 724 252Q725 250 724 246Q721 243 460 -56L196 -356Q196 -357 407 -357Q459 -357 548 -357T676 -358Q812 -358 896 -353T1063 -332T1204 -283T1307 -196Q1328 -170 1348 -124H1388Q1388 -125 1381 -145T1356 -210T1325 -294L1267 -449L666 -450Q64 -450 61 -448Q55 -446 55 -439Q55 -437 57 -433L590 177Q590 178 557 222T452 366T322 544L56 909L55 924Q55 945 60 948Z"></path><path stroke-width="0" id="E39-MJMATHI-69" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path stroke-width="0" id="E39-MJMAIN-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path stroke-width="0" id="E39-MJMATHI-79" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path stroke-width="0" id="E39-MJMAIN-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path stroke-width="0" id="E39-MJMAIN-5E" d="M112 560L249 694L257 686Q387 562 387 560L361 531Q359 532 303 581L250 627L195 580Q182 569 169 557T148 538L140 532Q138 530 125 546L112 560Z"></path><path stroke-width="0" id="E39-MJMAIN-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path><path stroke-width="0" id="E39-MJMAIN-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path stroke-width="0" id="E39-MJSZ4-221A" d="M983 1739Q988 1750 1001 1750Q1008 1750 1013 1745T1020 1733Q1020 1726 742 244T460 -1241Q458 -1250 439 -1250H436Q424 -1250 424 -1248L410 -1166Q395 -1083 367 -920T312 -601L201 44L137 -83L111 -57L187 96L264 247Q265 246 369 -357Q470 -958 473 -963L727 384Q979 1729 983 1739Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="#E39-MJMATHI-52" x="0" y="0"></use><use xlink:href="#E39-MJMATHI-4D" x="759" y="0"></use><use xlink:href="#E39-MJMATHI-53" x="1810" y="0"></use><use xlink:href="#E39-MJMATHI-45" x="2455" y="0"></use><use xlink:href="#E39-MJMAIN-3D" x="3496" y="0"></use><g transform="translate(4552,0)"><use xlink:href="#E39-MJSZ4-221A" x="0" y="-89"></use><rect stroke="none" width="6081" height="60" x="1000" y="1601"></rect><g transform="translate(1000,0)"><g transform="translate(120,0)"><rect stroke="none" width="720" height="60" x="0" y="220"></rect><use xlink:href="#E39-MJMAIN-31" x="110" y="676"></use><use xlink:href="#E39-MJMATHI-6E" x="60" y="-686"></use></g><g transform="translate(1126,0)"><use xlink:href="#E39-MJSZ2-2211" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="#E39-MJMATHI-69" x="848" y="-1534"></use><use transform="scale(0.707)" xlink:href="#E39-MJMATHI-6E" x="721" y="1626"></use></g><use xlink:href="#E39-MJMAIN-28" x="2570" y="0"></use><use xlink:href="#E39-MJMATHI-79" x="2959" y="0"></use><use xlink:href="#E39-MJMAIN-2212" x="3678" y="0"></use><g transform="translate(4679,0)"><use xlink:href="#E39-MJMATHI-79" x="1" y="0"></use><use xlink:href="#E39-MJMAIN-5E" x="60" y="-13"></use></g><g transform="translate(5239,0)"><use xlink:href="#E39-MJMAIN-29" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="#E39-MJMAIN-32" x="550" y="583"></use></g></g></g></g></svg></span></div><script type="math/tex; mode=display" id="MathJax-Element-10">RMSE = \sqrt{ \frac{1}{n} \sum_i^n(y - \hat{y})^2 }</script></div></div><p><span>where the variables are identical to the above. Note that the only different between MAE and RMSE is that RMSE first squares the difference between the true and predicted values, the takes the square root to return to the original units.</span></p><p><span>How do we choose between MAE and RMSE? We will get into further detail later in the course, but for now we can highlight key high-level trade-offs: </span><strong><span>MAE tends to be easier to interpret</span></strong><span>, aiding in human understanding and, if needed, correction. It is also generally robust to outliers. </span><strong><span>RMSE tends to allow for more efficient calculations</span></strong><span>, in large part because it is smoothly differentiable. It is, however, less robust to outliers. Which one is preferable in a given case is primarily an engineering question. Knowing the essential trade-offs, however, provide the foundations to engage in that conversation with your development team. (For accessible introductions to these topics, see </span><a href='https://medium.com/usf-msds/choosing-the-right-metric-for-machine-learning-models-part-1-a99d7d7414e4'><span>3</span></a><span> and </span><a href='https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d'><span>4</span></a><span>.)</span></p><h3><a name="classification" class="md-header-anchor"></a><span>Classification</span></h3><p><span>Competing classification metrics differ in surprisingly vital ways. All reflect goal specifications, frequently also reflecting implicit value judgments; this is because the metric for which a model is optimized suggests choosing one side of a trade-off over another. Classification metrics can bring policy and value considerations to the forefront. </span></p><h4><a name="the-confusion-matrix" class="md-header-anchor"></a><span>The confusion matrix</span></h4><p><span>The essential classification model metrics are anchored in a single table known as the </span><strong><span>confusion matrix</span></strong><span>. In the simple case of a binary classification, in which all data are labeled &quot;1&quot; or &quot;0&quot;, also known as &quot;positive&quot; or &quot;negative&quot;, the confusion matrix is as follows:</span></p><p>&nbsp;</p><table>
    <thead>
        <tr>
            <td colspan="1"></td>
            <td colspan="2" style="text-align:center;"><b>Actual Values</b><br>Positives: 1  &nbsp; Negatives: 0</td>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td rowspan="4"><b>Predicted Values</b><br>Positives: 1  &nbsp; Negatives: 0</td>
            <td rowspan="2"><b>True Positives (TP)</b><br><i>predicted: 1 &nbsp; actual: 1</i></td>
            <td><b>False Positives (FP)</b><br><i>predicted: 1 &nbsp; actual: 0</i></td>
        </tr>
        <tr>
            <!-- intentionally blank -->
        </tr>
        <tr>
            <td rowspan="4"><b>False Negatives (FN)</b><br><i>predicted: 0 &nbsp; actual: 1</i></td>
            <td rowspan="2"><b>True Negatives (TN)</b><br><i>predicted: 0 &nbsp; actual: 0</i></td>
        </tr>
        <tr>
        </tr>
    </tbody>
</table><p align="center">
    <i>Table 2:	The confusion matrix for a binary classification problem.
</i></p><ul><li><p><strong><span>Unpacking the confusion matrix</span></strong><span>: We first want to understand the differences between the four groups. </span><strong><span>TP</span></strong><span> and </span><strong><span>TN</span></strong><span> indicate </span><em><span>correct</span></em><span> predictions, while </span><strong><span>FP</span></strong><span> and </span><strong><span>FN</span></strong><span> indicate </span><em><span>incorrect</span></em><span> predictions. Why not have only two categories: correct and incorrect? In many cases, </span><strong><span>in matters in which direction we are wrong</span></strong><span>.</span></p><p><span>Consider again the loan problem. Would we rather deny applications unlikely to default, or approve applications likely to default?</span></p></li></ul><h4><a name="essential-metrics" class="md-header-anchor"></a><span>Essential Metrics</span></h4><p><span>With the confusion matrix in place, we can outline the four fundamental metrics drawn from it. (For further detail, see Mohammed Sunasra&#39;s excellent introduction: </span><a href='https://medium.com/thalus-ai/performance-metrics-for-classification-problems-in-machine-learning-part-i-b085d432082b'><span>X</span></a><span>.)As an example to contextualize these metrics, consider a classification algorithm designed to detect cancer. Assume a relatively low incidence in the given population - say, 0.5%.</span></p><ul><li><strong><span>Accuracy</span></strong><span>: measures percentage of total correct predictions: (TP * TN)/(TP + FP + FN + TN). This is a common metric for balanced datasets, but a very poor metric when a large majority of outcomes are in one class rather than another - for example, when one is low incidence, such as cancer in the assumed population.</span></li><li><strong><span>Precision</span></strong><span>: measures percentage of predicted positives that were correct: (TP)/(TP + FP). For example, of the people the model predicted to have cancer, how many actually had cancer?</span></li><li><strong><span>Sensitivity</span></strong><span>: measures percentage of actual positives that were correctly predicted: (TP)/(TP + FN). For example, of the people who actually had cancer, how many did we detect?</span></li><li><strong><span>Specificity</span></strong><span>: measures percentage of actual negatives that were correctly predicted: (TN)/(TN + FP). For example, of the people who did not actually have cancer, how many did we predict did not have cancer?</span></li></ul><p><span>These four metrics each entail trade-offs. What if your model requires risk aversion (e.g. high prediction of threats, even if including false alarms), but only up to a limit (not </span><em><span>too many</span></em><span> false alarms)?</span></p><p><span>For such real-world interests, combinations are often used to balance various trade-offs. The chief example, which we will study in a future session, is the </span><strong><span>AUC-ROC curve</span></strong><span>, which finds an optimal point between sensitivity and specificity (learn more about this curve at </span><a href='https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5'><span>5</span></a><span>):</span></p><p align="center">
    <img src="https://miro.medium.com/max/451/1*pk05QGzoWhCgRiiFbz-oKQ.png" alt="AUC-ROC curve" max-height="300"></p><p align="center">
    <i>Figure 3: A model AUC-ROC curve. Credit: Sarang Narkhede, Towards Data Science.
</i></p><ul><li><p><strong><span>The problem with accuracy in classification of imbalanced datasets</span></strong><span>: Let&#39;s return to the lending problem of situation 1a). Here, we almost certainly will have </span><em><span>imbalanced</span></em><span> data: far more instances of no default than of default. If only one in a hundred loan recipients defaulted in the training dataset, then no classification whatsoever would yield a 99% accuracy (correct classifications).</span></p><p><span>The problem is similar for a range of classification problems, from spam filters to cancer detection. The case of cancer detection makes clear yet another challenge, this time chiefly ethical: </span><strong><span>how risk tolerant should we be in detecting cancer</span></strong><span>?</span></p><p><span>Most respondents instinctively seek to minimize risk. However, when we are forced to put numbers to it, </span><em><span>minimal</span></em><span> risk suggests that if there is </span><em><span>any</span></em><span> percentage chance of cancer, we flag the case as cancerous, just in case. Few are likely to see zero percent chance, given the ambiguity and chance involved; meanwhile, cancer incidence is in reality quite low, likely less than one percent in healthy populations. This would move many if not most individuals to further rounds of testing. However, this is likely to be both impractical and not the ethically ideal scenario. What threshold, then, is acceptable, so that the hospital neither misses likely cases nor wantonly informs everyone they may have cancer? Part of the ethical decision is where to draw the line; another is to consider whether the model is sufficient to deploy at all. Both may have reasonable alternatives; but that is a decision that must be hard-coded, a joint effort by the policy personnel and the engineers. (For further discussion, see </span><a href='https://medium.com/thalus-ai/performance-metrics-for-classification-problems-in-machine-learning-part-i-b085d432082b'><span>6</span></a><span>).</span></p></li></ul><h3><a name="closing-comments" class="md-header-anchor"></a><span>Closing Comments</span></h3><p><span>Real-world applications of supervised learning are subject to an array of challenges and uncertainties. But with a critical awareness of data and model evaluation, such models can produce remarkable results. From a policy perspective, understanding the data requirements and available models for a given application empowers clear communication and coordination with engineers, helping to achieve the given objectives with appropriate trade-offs. Such communication and coordination are essential elements of effective AI policy and strategy.</span></p></div>
</body>
</html>